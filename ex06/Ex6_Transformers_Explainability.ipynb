{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws09dL1UiDRl"
   },
   "source": [
    "# Social Computing/Social Gaming - Summer 2023\n",
    "# Exercise Sheet 6 - Transformers and Explainable AI\n",
    "\n",
    "After we have worked out the base and social model on the previous sheet, we will now take a look into a model with a different approach. This time we will utilize Transformer-based classifiers. Transformers revolutionized NLP field significantly after the paper [\"Attention is all your need\"](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) was out. Then, a big [HuggingFace](https://huggingface.co/) platform [2] was created to store and host a lot of opensource NLP pre-trained models. We will work today with some of them.\n",
    "\n",
    "Furthermore we will make use of SHAP and its underlying Shapley values to understand the basics of neural network explainability. This [site](https://christophm.github.io/interpretable-ml-book/shapley.html) [4] will help you to get an understanding of this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "J0bpEcDOk5N9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so22nWqmCWt"
   },
   "source": [
    "## Task 6.0: The Data\n",
    "\n",
    "Once again we will use the dataset of Waseem and Hovy [1] as you are already familiar with it and it offers us the possibility to compare it to our previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmgC13RcmClz",
    "outputId": "2ce029a3-d3cb-4f64-b68b-f8744d895bf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/q9r9nqmx6_931g011m9qv1xw0000gn/T/ipykernel_74264/3071680186.py:2: DtypeWarning: Columns (2,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  waseem_hovy = pd.read_csv('./tweets.csv')\n"
     ]
    }
   ],
   "source": [
    "# Reads the data set from a .csv file\n",
    "waseem_hovy = pd.read_csv('./tweets.csv')\n",
    "waseem_hovy = waseem_hovy.astype(str)\n",
    "\n",
    "# This drop operation is necessary because of an inconsistency in the dataset\n",
    "waseem_hovy = waseem_hovy.drop([3343, 3344])\n",
    "waseem_hovy = waseem_hovy[['text', 'label']]\n",
    "\n",
    "# We need to do a unique and precise reordering to match with graph information later on\n",
    "unique_tweets, indices = np.unique(waseem_hovy['text'].to_numpy(), return_index=True)\n",
    "ordered_labels = waseem_hovy['label'].to_numpy()[indices]\n",
    "waseem_hovy = pd.DataFrame(np.stack((unique_tweets, ordered_labels), axis=1), columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHJCRCL7mMWa"
   },
   "source": [
    "## Task 6.1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeNLKRnUmXHm"
   },
   "source": [
    "### Encode the labels\n",
    "Since we are using the same dataset, we need to covert their textual representation into numerical. For this task (almost the same as for a previous sheet), we can use [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cVFTPmvxmD5T"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG_tQRAfmZHg",
    "outputId": "8971cb1b-e951-4c78-a445-445ad1986823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16849,)\n",
      "['none' 'racism' 'sexism']\n",
      "(16849,)\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Extract labels from dataset\n",
    "\n",
    "original_labels = np.array(waseem_hovy[\"label\"].tolist())\n",
    "\n",
    "# DONE:\n",
    "data_labels = LabelEncoder().fit_transform(original_labels)\n",
    "\n",
    "# Shows the actual shape of the labels\n",
    "print(original_labels.shape)\n",
    "print(np.unique(original_labels))\n",
    "print(data_labels.shape)\n",
    "print(np.unique(data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRd15wHr0fuA"
   },
   "source": [
    "In the case of transforfer usage, we do not need to preprocess and tokenize sentences by ourselves -- that will be done by model's tokenizer! So, let us dive into transfermers right now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrJi5ls8B0ki"
   },
   "source": [
    "## BERT for Toxic speech classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6p5COUW0tkk"
   },
   "source": [
    "We will fine-tune to our downstream task BERT model [3], more specifically -- distilled version of it [DistilBERT](https://huggingface.co/distilbert-base-uncased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QalZ-U8b1tpf"
   },
   "source": [
    "But, firstly, we need to install [transformers](https://huggingface.co/learn/nlp-course/chapter2/1?fw=pt) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XmkyLbV6nX47"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321,
     "referenced_widgets": [
      "0374cca38d90444696af2b524c1226df",
      "e680ef3aa5be4a489e16a08a8971da4c",
      "460f53bac5604ff2bce7b9725a488211",
      "daaa115de5c54fdbb00ff23e5da4dddc",
      "3c3ec381ed1f499b8919b58b4a19cbea",
      "7f78e904c343457282cd1ed660593e63",
      "0367c48ac9f248218a054aca2f4fc247",
      "60ba0c3288ac4d6c92b4a48755c3382a",
      "3e4bc097dfec4a84877635315cfc8c52",
      "fc0e30211ddb4876a94fab4a9060c6a0",
      "cb2586a6393d4dc9a2d37f178a84a0e2",
      "af1a87e298e548d5a661027cb61130c8",
      "c90d2a3614674d238b41c376b830b002",
      "b5f08dc0567c4bf388f202fe9140deb4",
      "6e940ab7ba71498387b9ca704bb32d33",
      "ee50296dc27e41e5ac60c01531c9c2ed",
      "38dc92e9f28a4b8a9bb4a6b3dc69e665",
      "ab18bbdffa844955a32a844a770a281a",
      "6e8c9d3c5ab341cea836b614e9c8724e",
      "38cdf8fa7ca94eaeb60cf119f266287f",
      "7bc00d075ea44470b7a6ab06f4f5d71b",
      "95af74c1855941d592b7e3b1df973741",
      "6306ea4bb3c3496da06a6bde10a01c3d",
      "b6fdf320928440088573398fee52539e",
      "33e752204a924671956108223cbd6c0c",
      "08e696f1f3fe4fa893fcd72e5f2a9d3c",
      "60328ca17fbc494ea138c1e119d786ee",
      "0867b040cc1f43b09a574f5a65a589fb",
      "5e4e67ad32b848fdbf3302806ac9e5b5",
      "973c4e60b05445aea213b5917fa235ae",
      "7839b4f86d7e4a20bd3dea4576b68980",
      "d1012bc04675458aa4d7e1b440072e43",
      "0af093b726be410e8ee82d6de450c0af",
      "ce2ad9f7e1e34124a3964b5cf191c5df",
      "c0ef2f3cb83f4bffb725a96357aef899",
      "d8cd8f2427884a79b4464ff3dfe919ab",
      "3463b61bbc534af4861d156df5b5dd59",
      "42d3b12688634ea0b14e96bad97680ea",
      "8d7c6352e9244a13816544b547a1295e",
      "942a0b515ddc4b95b9733282d87c8e48",
      "768fc0d1ceea4ffaa9cbab0ebbcb66b2",
      "6318439f0ee74d759f29da84767eb9b2",
      "2e3a3699dd6b4cd7b1ef055eb12942a6",
      "83b78202555d4f029c221b6370a4b4ca"
     ]
    },
    "id": "-5HMYYG3CKvr",
    "outputId": "3a5e166c-58c7-4837-979f-cca33d30ecc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'classifier.weight', 'pooler.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# DONE: initialize tokenizer and model for DistilBERT or\n",
    "# for other model of your preferences -- you are very welcome to try out something different!\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSeIwKOaC5K7"
   },
   "source": [
    "### Training Batches Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0KbvPJI2sTF"
   },
   "source": [
    "The same as in the previous tutorial, we will create our custom datasets and loaders to generate batches for training. However, we need to adapt it to transformers input:\n",
    "\n",
    "1.   Each dataset item should return ``input_ids``, ``attention_mask``, and ``label``.\n",
    "2.   All should be ``tensors``.\n",
    "3.   In the end, you need to apply ``collate_fn`` -- that will pad all tensors in batches to the max_length (already implemented for you).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "o-AVZWqWDi1V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KRchF0ABN1Ov"
   },
   "outputs": [],
   "source": [
    "# this is a special function that pad sequences in the batch for data loader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True)\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'label': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5kSezoJtCYho"
   },
   "outputs": [],
   "source": [
    "# DONE: create your CustomDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        # DONE\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        # DONE\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # DONE\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[index],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(self.labels[index])\n",
    "        }\n",
    "\n",
    "# ––––––––––––––– End of Solution –––––––––––––––––––"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9xEfmt_EQSK",
    "outputId": "69ee32f6-71c1-4136-90de-514f1ff4734e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (10109,), Labels shape: (10109,)\n",
      "Test data shape: (3370,), Labels shape: (3370,)\n",
      "Validation data shape: (3370,), Labels shape: (3370,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = waseem_hovy['text'].values\n",
    "\n",
    "# DONE: Split tweets and labels in Train/Test/Validation 60/20/20\n",
    "X_train, temp_texts, y_train, temp_labels = train_test_split(texts, data_labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training data shape: {}, Labels shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test data shape: {}, Labels shape: {}\".format(X_test.shape, y_test.shape))\n",
    "print(\"Validation data shape: {}, Labels shape: {}\".format(X_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bGN9XaYREak4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create the Datasets\n",
    "train_dataset = CustomDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = CustomDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = CustomDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# DataLoader for batching and parallel data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZSccwA-C8xm"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iswJNFaU38om"
   },
   "source": [
    "We are ready to train the model! You can reuse the code from the previous tutorial:\n",
    "1. Define ``optimizer`` and ``criterion`` for a classiication task.\n",
    "2. Use ``train_lodaer`` to sample batches for training.\n",
    "3. Track validation loss using data from ``val_loader``.\n",
    "4. Achtung: now the items in batches have different structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HkVJeXrIE3a3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4ECkAeuWEmOY"
   },
   "outputs": [],
   "source": [
    "# DONE: define optimizer and criterion\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOKzq--OC901",
    "outputId": "01c4711f-f4c6-43d3-97a7-93782991faec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/316], Loss: 0.7053\n",
      "Epoch [1/3], Step [2/316], Loss: 0.7314\n",
      "Epoch [1/3], Step [3/316], Loss: 0.9537\n",
      "Epoch [1/3], Step [4/316], Loss: 1.1709\n",
      "Epoch [1/3], Step [5/316], Loss: 0.8197\n",
      "Epoch [1/3], Step [6/316], Loss: 0.7228\n",
      "Epoch [1/3], Step [7/316], Loss: 0.7301\n",
      "Epoch [1/3], Step [8/316], Loss: 0.7684\n",
      "Epoch [1/3], Step [9/316], Loss: 0.8017\n",
      "Epoch [1/3], Step [10/316], Loss: 0.7866\n",
      "Epoch [1/3], Step [11/316], Loss: 0.8603\n",
      "Epoch [1/3], Step [12/316], Loss: 0.6776\n",
      "Epoch [1/3], Step [13/316], Loss: 1.0044\n",
      "Epoch [1/3], Step [14/316], Loss: 0.9233\n",
      "Epoch [1/3], Step [15/316], Loss: 0.6710\n",
      "Epoch [1/3], Step [16/316], Loss: 0.7234\n",
      "Epoch [1/3], Step [17/316], Loss: 0.7442\n",
      "Epoch [1/3], Step [18/316], Loss: 0.8716\n",
      "Epoch [1/3], Step [19/316], Loss: 0.7289\n",
      "Epoch [1/3], Step [20/316], Loss: 0.6748\n",
      "Epoch [1/3], Step [21/316], Loss: 0.9503\n",
      "Epoch [1/3], Step [22/316], Loss: 0.9624\n",
      "Epoch [1/3], Step [23/316], Loss: 0.6941\n",
      "Epoch [1/3], Step [24/316], Loss: 0.7773\n",
      "Epoch [1/3], Step [25/316], Loss: 0.8564\n",
      "Epoch [1/3], Step [26/316], Loss: 0.8968\n",
      "Epoch [1/3], Step [27/316], Loss: 0.8348\n",
      "Epoch [1/3], Step [28/316], Loss: 0.7269\n",
      "Epoch [1/3], Step [29/316], Loss: 0.7985\n",
      "Epoch [1/3], Step [30/316], Loss: 0.8022\n",
      "Epoch [1/3], Step [31/316], Loss: 0.9997\n",
      "Epoch [1/3], Step [32/316], Loss: 0.9793\n",
      "Epoch [1/3], Step [33/316], Loss: 0.7853\n",
      "Epoch [1/3], Step [34/316], Loss: 0.6885\n",
      "Epoch [1/3], Step [35/316], Loss: 0.8786\n",
      "Epoch [1/3], Step [36/316], Loss: 0.8941\n",
      "Epoch [1/3], Step [37/316], Loss: 0.7742\n",
      "Epoch [1/3], Step [38/316], Loss: 0.8702\n",
      "Epoch [1/3], Step [39/316], Loss: 0.6905\n",
      "Epoch [1/3], Step [40/316], Loss: 0.9843\n",
      "Epoch [1/3], Step [41/316], Loss: 0.9760\n",
      "Epoch [1/3], Step [42/316], Loss: 0.8062\n",
      "Epoch [1/3], Step [43/316], Loss: 0.7493\n",
      "Epoch [1/3], Step [44/316], Loss: 0.9467\n",
      "Epoch [1/3], Step [45/316], Loss: 0.8349\n",
      "Epoch [1/3], Step [46/316], Loss: 0.9195\n",
      "Epoch [1/3], Step [47/316], Loss: 0.8548\n",
      "Epoch [1/3], Step [48/316], Loss: 0.9145\n",
      "Epoch [1/3], Step [49/316], Loss: 0.9083\n",
      "Epoch [1/3], Step [50/316], Loss: 0.8211\n",
      "Epoch [1/3], Step [51/316], Loss: 0.7893\n",
      "Epoch [1/3], Step [52/316], Loss: 0.8032\n",
      "Epoch [1/3], Step [53/316], Loss: 0.8025\n",
      "Epoch [1/3], Step [54/316], Loss: 0.6848\n",
      "Epoch [1/3], Step [55/316], Loss: 0.8331\n",
      "Epoch [1/3], Step [56/316], Loss: 0.7783\n",
      "Epoch [1/3], Step [57/316], Loss: 0.9097\n",
      "Epoch [1/3], Step [58/316], Loss: 0.8094\n",
      "Epoch [1/3], Step [59/316], Loss: 0.8262\n",
      "Epoch [1/3], Step [60/316], Loss: 0.8919\n",
      "Epoch [1/3], Step [61/316], Loss: 0.8696\n",
      "Epoch [1/3], Step [62/316], Loss: 0.9691\n",
      "Epoch [1/3], Step [63/316], Loss: 0.8549\n",
      "Epoch [1/3], Step [64/316], Loss: 0.8497\n",
      "Epoch [1/3], Step [65/316], Loss: 0.7894\n",
      "Epoch [1/3], Step [66/316], Loss: 0.9702\n",
      "Epoch [1/3], Step [67/316], Loss: 0.7229\n",
      "Epoch [1/3], Step [68/316], Loss: 0.8495\n",
      "Epoch [1/3], Step [69/316], Loss: 0.8192\n",
      "Epoch [1/3], Step [70/316], Loss: 0.9490\n",
      "Epoch [1/3], Step [71/316], Loss: 0.9275\n",
      "Epoch [1/3], Step [72/316], Loss: 0.7853\n",
      "Epoch [1/3], Step [73/316], Loss: 0.9073\n",
      "Epoch [1/3], Step [74/316], Loss: 0.7793\n",
      "Epoch [1/3], Step [75/316], Loss: 0.8016\n",
      "Epoch [1/3], Step [76/316], Loss: 0.6909\n",
      "Epoch [1/3], Step [77/316], Loss: 0.4816\n",
      "Epoch [1/3], Step [78/316], Loss: 0.8651\n",
      "Epoch [1/3], Step [79/316], Loss: 0.6821\n",
      "Epoch [1/3], Step [80/316], Loss: 0.8002\n",
      "Epoch [1/3], Step [81/316], Loss: 0.6927\n",
      "Epoch [1/3], Step [82/316], Loss: 0.8295\n",
      "Epoch [1/3], Step [83/316], Loss: 0.4502\n",
      "Epoch [1/3], Step [84/316], Loss: 0.8965\n",
      "Epoch [1/3], Step [85/316], Loss: 0.8174\n",
      "Epoch [1/3], Step [86/316], Loss: 0.7440\n",
      "Epoch [1/3], Step [87/316], Loss: 0.8382\n",
      "Epoch [1/3], Step [88/316], Loss: 0.7836\n",
      "Epoch [1/3], Step [89/316], Loss: 0.8613\n",
      "Epoch [1/3], Step [90/316], Loss: 0.7966\n",
      "Epoch [1/3], Step [91/316], Loss: 0.8655\n",
      "Epoch [1/3], Step [92/316], Loss: 0.8663\n",
      "Epoch [1/3], Step [93/316], Loss: 0.9259\n",
      "Epoch [1/3], Step [94/316], Loss: 0.8330\n",
      "Epoch [1/3], Step [95/316], Loss: 0.6768\n",
      "Epoch [1/3], Step [96/316], Loss: 0.7541\n",
      "Epoch [1/3], Step [97/316], Loss: 0.6140\n",
      "Epoch [1/3], Step [98/316], Loss: 0.6905\n",
      "Epoch [1/3], Step [99/316], Loss: 1.0759\n",
      "Epoch [1/3], Step [100/316], Loss: 1.0127\n",
      "Epoch [1/3], Step [101/316], Loss: 0.7666\n",
      "Epoch [1/3], Step [102/316], Loss: 0.9418\n",
      "Epoch [1/3], Step [103/316], Loss: 0.7874\n",
      "Epoch [1/3], Step [104/316], Loss: 1.0845\n",
      "Epoch [1/3], Step [105/316], Loss: 0.8601\n",
      "Epoch [1/3], Step [106/316], Loss: 1.0044\n",
      "Epoch [1/3], Step [107/316], Loss: 0.9418\n",
      "Epoch [1/3], Step [108/316], Loss: 0.8891\n",
      "Epoch [1/3], Step [109/316], Loss: 0.8424\n",
      "Epoch [1/3], Step [110/316], Loss: 0.9370\n",
      "Epoch [1/3], Step [111/316], Loss: 0.8898\n",
      "Epoch [1/3], Step [112/316], Loss: 0.7143\n",
      "Epoch [1/3], Step [113/316], Loss: 0.6925\n",
      "Epoch [1/3], Step [114/316], Loss: 0.6550\n",
      "Epoch [1/3], Step [115/316], Loss: 0.7135\n",
      "Epoch [1/3], Step [116/316], Loss: 0.6738\n",
      "Epoch [1/3], Step [117/316], Loss: 1.1330\n",
      "Epoch [1/3], Step [118/316], Loss: 0.6840\n",
      "Epoch [1/3], Step [119/316], Loss: 0.5759\n",
      "Epoch [1/3], Step [120/316], Loss: 0.6423\n",
      "Epoch [1/3], Step [121/316], Loss: 0.8417\n",
      "Epoch [1/3], Step [122/316], Loss: 0.8532\n",
      "Epoch [1/3], Step [123/316], Loss: 1.0473\n",
      "Epoch [1/3], Step [124/316], Loss: 0.7338\n",
      "Epoch [1/3], Step [125/316], Loss: 1.0085\n",
      "Epoch [1/3], Step [126/316], Loss: 0.7276\n",
      "Epoch [1/3], Step [127/316], Loss: 0.9665\n",
      "Epoch [1/3], Step [128/316], Loss: 0.8842\n",
      "Epoch [1/3], Step [129/316], Loss: 0.6063\n",
      "Epoch [1/3], Step [130/316], Loss: 0.7737\n",
      "Epoch [1/3], Step [131/316], Loss: 1.0909\n",
      "Epoch [1/3], Step [132/316], Loss: 0.8129\n",
      "Epoch [1/3], Step [133/316], Loss: 0.6430\n",
      "Epoch [1/3], Step [134/316], Loss: 0.6434\n",
      "Epoch [1/3], Step [135/316], Loss: 0.6429\n",
      "Epoch [1/3], Step [136/316], Loss: 0.7450\n",
      "Epoch [1/3], Step [137/316], Loss: 0.5172\n",
      "Epoch [1/3], Step [138/316], Loss: 0.9149\n",
      "Epoch [1/3], Step [139/316], Loss: 0.7877\n",
      "Epoch [1/3], Step [140/316], Loss: 0.9093\n",
      "Epoch [1/3], Step [141/316], Loss: 0.7438\n",
      "Epoch [1/3], Step [142/316], Loss: 0.7763\n",
      "Epoch [1/3], Step [143/316], Loss: 0.7705\n",
      "Epoch [1/3], Step [144/316], Loss: 0.7527\n",
      "Epoch [1/3], Step [145/316], Loss: 0.5260\n",
      "Epoch [1/3], Step [146/316], Loss: 0.8882\n",
      "Epoch [1/3], Step [147/316], Loss: 0.8281\n",
      "Epoch [1/3], Step [148/316], Loss: 0.5651\n",
      "Epoch [1/3], Step [149/316], Loss: 1.0039\n",
      "Epoch [1/3], Step [150/316], Loss: 0.6233\n",
      "Epoch [1/3], Step [151/316], Loss: 0.6929\n",
      "Epoch [1/3], Step [152/316], Loss: 0.7640\n",
      "Epoch [1/3], Step [153/316], Loss: 0.8516\n",
      "Epoch [1/3], Step [154/316], Loss: 0.7757\n",
      "Epoch [1/3], Step [155/316], Loss: 0.8984\n",
      "Epoch [1/3], Step [156/316], Loss: 0.7749\n",
      "Epoch [1/3], Step [157/316], Loss: 0.7437\n",
      "Epoch [1/3], Step [158/316], Loss: 0.6614\n",
      "Epoch [1/3], Step [159/316], Loss: 0.6168\n",
      "Epoch [1/3], Step [160/316], Loss: 0.5813\n",
      "Epoch [1/3], Step [161/316], Loss: 0.6187\n",
      "Epoch [1/3], Step [162/316], Loss: 0.7375\n",
      "Epoch [1/3], Step [163/316], Loss: 0.8268\n",
      "Epoch [1/3], Step [164/316], Loss: 1.2278\n",
      "Epoch [1/3], Step [165/316], Loss: 1.0066\n",
      "Epoch [1/3], Step [166/316], Loss: 0.7172\n",
      "Epoch [1/3], Step [167/316], Loss: 0.5750\n",
      "Epoch [1/3], Step [168/316], Loss: 0.8664\n",
      "Epoch [1/3], Step [169/316], Loss: 0.7226\n",
      "Epoch [1/3], Step [170/316], Loss: 0.8644\n",
      "Epoch [1/3], Step [171/316], Loss: 0.6383\n",
      "Epoch [1/3], Step [172/316], Loss: 0.7636\n",
      "Epoch [1/3], Step [173/316], Loss: 0.7301\n",
      "Epoch [1/3], Step [174/316], Loss: 0.7615\n",
      "Epoch [1/3], Step [175/316], Loss: 0.8459\n",
      "Epoch [1/3], Step [176/316], Loss: 0.7474\n",
      "Epoch [1/3], Step [177/316], Loss: 0.6414\n",
      "Epoch [1/3], Step [178/316], Loss: 0.6364\n",
      "Epoch [1/3], Step [179/316], Loss: 0.5123\n",
      "Epoch [1/3], Step [180/316], Loss: 0.6506\n",
      "Epoch [1/3], Step [181/316], Loss: 0.7959\n",
      "Epoch [1/3], Step [182/316], Loss: 0.5082\n",
      "Epoch [1/3], Step [183/316], Loss: 0.6520\n",
      "Epoch [1/3], Step [184/316], Loss: 0.6793\n",
      "Epoch [1/3], Step [185/316], Loss: 0.5446\n",
      "Epoch [1/3], Step [186/316], Loss: 0.6002\n",
      "Epoch [1/3], Step [187/316], Loss: 0.7654\n",
      "Epoch [1/3], Step [188/316], Loss: 0.7871\n",
      "Epoch [1/3], Step [189/316], Loss: 0.6474\n",
      "Epoch [1/3], Step [190/316], Loss: 0.6281\n",
      "Epoch [1/3], Step [191/316], Loss: 0.6037\n",
      "Epoch [1/3], Step [192/316], Loss: 0.8182\n",
      "Epoch [1/3], Step [193/316], Loss: 0.5308\n",
      "Epoch [1/3], Step [194/316], Loss: 0.5363\n",
      "Epoch [1/3], Step [195/316], Loss: 0.6386\n",
      "Epoch [1/3], Step [196/316], Loss: 0.7729\n",
      "Epoch [1/3], Step [197/316], Loss: 0.6006\n",
      "Epoch [1/3], Step [198/316], Loss: 0.4170\n",
      "Epoch [1/3], Step [199/316], Loss: 0.7922\n",
      "Epoch [1/3], Step [200/316], Loss: 0.9863\n",
      "Epoch [1/3], Step [201/316], Loss: 0.9515\n",
      "Epoch [1/3], Step [202/316], Loss: 0.5460\n",
      "Epoch [1/3], Step [203/316], Loss: 0.7788\n",
      "Epoch [1/3], Step [204/316], Loss: 0.7530\n",
      "Epoch [1/3], Step [205/316], Loss: 0.5758\n",
      "Epoch [1/3], Step [206/316], Loss: 0.5670\n",
      "Epoch [1/3], Step [207/316], Loss: 0.8256\n",
      "Epoch [1/3], Step [208/316], Loss: 0.6762\n",
      "Epoch [1/3], Step [209/316], Loss: 0.4819\n",
      "Epoch [1/3], Step [210/316], Loss: 0.6247\n",
      "Epoch [1/3], Step [211/316], Loss: 0.7050\n",
      "Epoch [1/3], Step [212/316], Loss: 0.6380\n",
      "Epoch [1/3], Step [213/316], Loss: 0.6399\n",
      "Epoch [1/3], Step [214/316], Loss: 0.7021\n",
      "Epoch [1/3], Step [215/316], Loss: 0.5078\n",
      "Epoch [1/3], Step [216/316], Loss: 0.4978\n",
      "Epoch [1/3], Step [217/316], Loss: 0.6945\n",
      "Epoch [1/3], Step [218/316], Loss: 0.6947\n",
      "Epoch [1/3], Step [219/316], Loss: 0.5632\n",
      "Epoch [1/3], Step [220/316], Loss: 0.6610\n",
      "Epoch [1/3], Step [221/316], Loss: 0.4399\n",
      "Epoch [1/3], Step [222/316], Loss: 0.7717\n",
      "Epoch [1/3], Step [223/316], Loss: 0.4792\n",
      "Epoch [1/3], Step [224/316], Loss: 0.6530\n",
      "Epoch [1/3], Step [225/316], Loss: 0.5476\n",
      "Epoch [1/3], Step [226/316], Loss: 0.5327\n",
      "Epoch [1/3], Step [227/316], Loss: 0.3832\n",
      "Epoch [1/3], Step [228/316], Loss: 0.7504\n",
      "Epoch [1/3], Step [229/316], Loss: 0.6255\n",
      "Epoch [1/3], Step [230/316], Loss: 0.4308\n",
      "Epoch [1/3], Step [231/316], Loss: 0.6227\n",
      "Epoch [1/3], Step [232/316], Loss: 0.4946\n",
      "Epoch [1/3], Step [233/316], Loss: 0.7775\n",
      "Epoch [1/3], Step [234/316], Loss: 0.6985\n",
      "Epoch [1/3], Step [235/316], Loss: 0.4380\n",
      "Epoch [1/3], Step [236/316], Loss: 0.7817\n",
      "Epoch [1/3], Step [237/316], Loss: 0.6074\n",
      "Epoch [1/3], Step [238/316], Loss: 0.5725\n",
      "Epoch [1/3], Step [239/316], Loss: 0.5006\n",
      "Epoch [1/3], Step [240/316], Loss: 0.5935\n",
      "Epoch [1/3], Step [241/316], Loss: 0.5131\n",
      "Epoch [1/3], Step [242/316], Loss: 0.3574\n",
      "Epoch [1/3], Step [243/316], Loss: 0.6728\n",
      "Epoch [1/3], Step [244/316], Loss: 0.6019\n",
      "Epoch [1/3], Step [245/316], Loss: 0.4729\n",
      "Epoch [1/3], Step [246/316], Loss: 0.3811\n",
      "Epoch [1/3], Step [247/316], Loss: 0.4576\n",
      "Epoch [1/3], Step [248/316], Loss: 0.7408\n",
      "Epoch [1/3], Step [249/316], Loss: 0.5660\n",
      "Epoch [1/3], Step [250/316], Loss: 0.5067\n",
      "Epoch [1/3], Step [251/316], Loss: 0.5597\n",
      "Epoch [1/3], Step [252/316], Loss: 0.4643\n",
      "Epoch [1/3], Step [253/316], Loss: 0.4918\n",
      "Epoch [1/3], Step [254/316], Loss: 0.5690\n",
      "Epoch [1/3], Step [255/316], Loss: 0.2496\n",
      "Epoch [1/3], Step [256/316], Loss: 0.5892\n",
      "Epoch [1/3], Step [257/316], Loss: 0.4762\n",
      "Epoch [1/3], Step [258/316], Loss: 0.4312\n",
      "Epoch [1/3], Step [259/316], Loss: 0.5062\n",
      "Epoch [1/3], Step [260/316], Loss: 0.5165\n",
      "Epoch [1/3], Step [261/316], Loss: 0.7009\n",
      "Epoch [1/3], Step [262/316], Loss: 0.5180\n",
      "Epoch [1/3], Step [263/316], Loss: 0.6804\n",
      "Epoch [1/3], Step [264/316], Loss: 0.5860\n",
      "Epoch [1/3], Step [265/316], Loss: 0.6120\n",
      "Epoch [1/3], Step [266/316], Loss: 0.6690\n",
      "Epoch [1/3], Step [267/316], Loss: 0.4742\n",
      "Epoch [1/3], Step [268/316], Loss: 0.3758\n",
      "Epoch [1/3], Step [269/316], Loss: 0.6796\n",
      "Epoch [1/3], Step [270/316], Loss: 0.5842\n",
      "Epoch [1/3], Step [271/316], Loss: 0.4917\n",
      "Epoch [1/3], Step [272/316], Loss: 0.4072\n",
      "Epoch [1/3], Step [273/316], Loss: 0.4804\n",
      "Epoch [1/3], Step [274/316], Loss: 0.4151\n",
      "Epoch [1/3], Step [275/316], Loss: 0.7590\n",
      "Epoch [1/3], Step [276/316], Loss: 0.7641\n",
      "Epoch [1/3], Step [277/316], Loss: 0.7027\n",
      "Epoch [1/3], Step [278/316], Loss: 0.6102\n",
      "Epoch [1/3], Step [279/316], Loss: 0.4106\n",
      "Epoch [1/3], Step [280/316], Loss: 0.4795\n",
      "Epoch [1/3], Step [281/316], Loss: 0.5087\n",
      "Epoch [1/3], Step [282/316], Loss: 0.6611\n",
      "Epoch [1/3], Step [283/316], Loss: 0.6480\n",
      "Epoch [1/3], Step [284/316], Loss: 0.8621\n",
      "Epoch [1/3], Step [285/316], Loss: 0.6308\n",
      "Epoch [1/3], Step [286/316], Loss: 0.7189\n",
      "Epoch [1/3], Step [287/316], Loss: 0.6539\n",
      "Epoch [1/3], Step [288/316], Loss: 0.5738\n",
      "Epoch [1/3], Step [289/316], Loss: 0.5065\n",
      "Epoch [1/3], Step [290/316], Loss: 0.4673\n",
      "Epoch [1/3], Step [291/316], Loss: 0.4503\n",
      "Epoch [1/3], Step [292/316], Loss: 0.6868\n",
      "Epoch [1/3], Step [293/316], Loss: 0.5665\n",
      "Epoch [1/3], Step [294/316], Loss: 0.7501\n",
      "Epoch [1/3], Step [295/316], Loss: 0.5119\n",
      "Epoch [1/3], Step [296/316], Loss: 0.6258\n",
      "Epoch [1/3], Step [297/316], Loss: 0.4941\n",
      "Epoch [1/3], Step [298/316], Loss: 0.5205\n",
      "Epoch [1/3], Step [299/316], Loss: 0.7402\n",
      "Epoch [1/3], Step [300/316], Loss: 0.4865\n",
      "Epoch [1/3], Step [301/316], Loss: 0.7826\n",
      "Epoch [1/3], Step [302/316], Loss: 0.4191\n",
      "Epoch [1/3], Step [303/316], Loss: 0.6663\n",
      "Epoch [1/3], Step [304/316], Loss: 0.3381\n",
      "Epoch [1/3], Step [305/316], Loss: 0.5873\n",
      "Epoch [1/3], Step [306/316], Loss: 0.3579\n",
      "Epoch [1/3], Step [307/316], Loss: 0.6650\n",
      "Epoch [1/3], Step [308/316], Loss: 0.3700\n",
      "Epoch [1/3], Step [309/316], Loss: 0.4815\n",
      "Epoch [1/3], Step [310/316], Loss: 0.7812\n",
      "Epoch [1/3], Step [311/316], Loss: 0.6000\n",
      "Epoch [1/3], Step [312/316], Loss: 0.4444\n",
      "Epoch [1/3], Step [313/316], Loss: 0.8003\n",
      "Epoch [1/3], Step [314/316], Loss: 0.6434\n",
      "Epoch [1/3], Step [315/316], Loss: 0.5418\n",
      "Epoch [1/3], Step [316/316], Loss: 0.7688\n",
      "Epoch [1/3], Train Loss: 0.7139\n",
      "Epoch [1/3], Validation Accuracy: 0.7703\n",
      "Epoch [2/3], Step [1/316], Loss: 0.4017\n",
      "Epoch [2/3], Step [2/316], Loss: 0.5327\n",
      "Epoch [2/3], Step [3/316], Loss: 0.4676\n",
      "Epoch [2/3], Step [4/316], Loss: 0.5925\n",
      "Epoch [2/3], Step [5/316], Loss: 0.4240\n",
      "Epoch [2/3], Step [6/316], Loss: 0.3525\n",
      "Epoch [2/3], Step [7/316], Loss: 0.3052\n",
      "Epoch [2/3], Step [8/316], Loss: 0.5067\n",
      "Epoch [2/3], Step [9/316], Loss: 0.6652\n",
      "Epoch [2/3], Step [10/316], Loss: 0.4610\n",
      "Epoch [2/3], Step [11/316], Loss: 0.5898\n",
      "Epoch [2/3], Step [12/316], Loss: 0.4507\n",
      "Epoch [2/3], Step [13/316], Loss: 0.4952\n",
      "Epoch [2/3], Step [14/316], Loss: 0.5235\n",
      "Epoch [2/3], Step [15/316], Loss: 0.7102\n",
      "Epoch [2/3], Step [16/316], Loss: 0.4490\n",
      "Epoch [2/3], Step [17/316], Loss: 0.5765\n",
      "Epoch [2/3], Step [18/316], Loss: 0.4227\n",
      "Epoch [2/3], Step [19/316], Loss: 0.4943\n",
      "Epoch [2/3], Step [20/316], Loss: 0.8006\n",
      "Epoch [2/3], Step [21/316], Loss: 0.5839\n",
      "Epoch [2/3], Step [22/316], Loss: 0.3088\n",
      "Epoch [2/3], Step [23/316], Loss: 0.3128\n",
      "Epoch [2/3], Step [24/316], Loss: 0.5078\n",
      "Epoch [2/3], Step [25/316], Loss: 0.3789\n",
      "Epoch [2/3], Step [26/316], Loss: 0.4140\n",
      "Epoch [2/3], Step [27/316], Loss: 0.3434\n",
      "Epoch [2/3], Step [28/316], Loss: 0.3125\n",
      "Epoch [2/3], Step [29/316], Loss: 0.4700\n",
      "Epoch [2/3], Step [30/316], Loss: 0.4044\n",
      "Epoch [2/3], Step [31/316], Loss: 0.3074\n",
      "Epoch [2/3], Step [32/316], Loss: 0.4509\n",
      "Epoch [2/3], Step [33/316], Loss: 0.2322\n",
      "Epoch [2/3], Step [34/316], Loss: 0.4724\n",
      "Epoch [2/3], Step [35/316], Loss: 0.4795\n",
      "Epoch [2/3], Step [36/316], Loss: 0.3545\n",
      "Epoch [2/3], Step [37/316], Loss: 0.2125\n",
      "Epoch [2/3], Step [38/316], Loss: 0.8452\n",
      "Epoch [2/3], Step [39/316], Loss: 0.2966\n",
      "Epoch [2/3], Step [40/316], Loss: 0.3668\n",
      "Epoch [2/3], Step [41/316], Loss: 0.6399\n",
      "Epoch [2/3], Step [42/316], Loss: 0.6835\n",
      "Epoch [2/3], Step [43/316], Loss: 0.3040\n",
      "Epoch [2/3], Step [44/316], Loss: 0.7086\n",
      "Epoch [2/3], Step [45/316], Loss: 0.5201\n",
      "Epoch [2/3], Step [46/316], Loss: 0.4979\n",
      "Epoch [2/3], Step [47/316], Loss: 1.0457\n",
      "Epoch [2/3], Step [48/316], Loss: 0.7562\n",
      "Epoch [2/3], Step [49/316], Loss: 0.2982\n",
      "Epoch [2/3], Step [50/316], Loss: 0.7270\n",
      "Epoch [2/3], Step [51/316], Loss: 0.4789\n",
      "Epoch [2/3], Step [52/316], Loss: 0.6644\n",
      "Epoch [2/3], Step [53/316], Loss: 0.2937\n",
      "Epoch [2/3], Step [54/316], Loss: 0.5885\n",
      "Epoch [2/3], Step [55/316], Loss: 0.6324\n",
      "Epoch [2/3], Step [56/316], Loss: 0.5664\n",
      "Epoch [2/3], Step [57/316], Loss: 0.4870\n",
      "Epoch [2/3], Step [58/316], Loss: 0.4785\n",
      "Epoch [2/3], Step [59/316], Loss: 0.6925\n",
      "Epoch [2/3], Step [60/316], Loss: 0.3875\n",
      "Epoch [2/3], Step [61/316], Loss: 0.4091\n",
      "Epoch [2/3], Step [62/316], Loss: 0.4013\n",
      "Epoch [2/3], Step [63/316], Loss: 0.4539\n",
      "Epoch [2/3], Step [64/316], Loss: 0.4998\n",
      "Epoch [2/3], Step [65/316], Loss: 0.3180\n",
      "Epoch [2/3], Step [66/316], Loss: 0.3804\n",
      "Epoch [2/3], Step [67/316], Loss: 0.4438\n",
      "Epoch [2/3], Step [68/316], Loss: 0.4424\n",
      "Epoch [2/3], Step [69/316], Loss: 0.4555\n",
      "Epoch [2/3], Step [70/316], Loss: 0.3842\n",
      "Epoch [2/3], Step [71/316], Loss: 0.3010\n",
      "Epoch [2/3], Step [72/316], Loss: 0.5502\n",
      "Epoch [2/3], Step [73/316], Loss: 0.6659\n",
      "Epoch [2/3], Step [74/316], Loss: 0.6610\n",
      "Epoch [2/3], Step [75/316], Loss: 0.5559\n",
      "Epoch [2/3], Step [76/316], Loss: 0.3494\n",
      "Epoch [2/3], Step [77/316], Loss: 0.5189\n",
      "Epoch [2/3], Step [78/316], Loss: 0.5449\n",
      "Epoch [2/3], Step [79/316], Loss: 0.4162\n",
      "Epoch [2/3], Step [80/316], Loss: 0.4800\n",
      "Epoch [2/3], Step [81/316], Loss: 0.2136\n",
      "Epoch [2/3], Step [82/316], Loss: 0.4815\n",
      "Epoch [2/3], Step [83/316], Loss: 0.3204\n",
      "Epoch [2/3], Step [84/316], Loss: 0.4901\n",
      "Epoch [2/3], Step [85/316], Loss: 0.3760\n",
      "Epoch [2/3], Step [86/316], Loss: 0.2101\n",
      "Epoch [2/3], Step [87/316], Loss: 0.3050\n",
      "Epoch [2/3], Step [88/316], Loss: 0.2478\n",
      "Epoch [2/3], Step [89/316], Loss: 0.5678\n",
      "Epoch [2/3], Step [90/316], Loss: 0.3527\n",
      "Epoch [2/3], Step [91/316], Loss: 0.4029\n",
      "Epoch [2/3], Step [92/316], Loss: 0.4586\n",
      "Epoch [2/3], Step [93/316], Loss: 0.3482\n",
      "Epoch [2/3], Step [94/316], Loss: 0.7111\n",
      "Epoch [2/3], Step [95/316], Loss: 0.5576\n",
      "Epoch [2/3], Step [96/316], Loss: 0.6263\n",
      "Epoch [2/3], Step [97/316], Loss: 0.4325\n",
      "Epoch [2/3], Step [98/316], Loss: 0.6396\n",
      "Epoch [2/3], Step [99/316], Loss: 0.5130\n",
      "Epoch [2/3], Step [100/316], Loss: 0.4785\n",
      "Epoch [2/3], Step [101/316], Loss: 0.2780\n",
      "Epoch [2/3], Step [102/316], Loss: 0.5817\n",
      "Epoch [2/3], Step [103/316], Loss: 0.3928\n",
      "Epoch [2/3], Step [104/316], Loss: 0.3831\n",
      "Epoch [2/3], Step [105/316], Loss: 0.2884\n",
      "Epoch [2/3], Step [106/316], Loss: 0.4556\n",
      "Epoch [2/3], Step [107/316], Loss: 0.3578\n",
      "Epoch [2/3], Step [108/316], Loss: 0.4121\n",
      "Epoch [2/3], Step [109/316], Loss: 0.3640\n",
      "Epoch [2/3], Step [110/316], Loss: 0.4819\n",
      "Epoch [2/3], Step [111/316], Loss: 0.6654\n",
      "Epoch [2/3], Step [112/316], Loss: 0.4444\n",
      "Epoch [2/3], Step [113/316], Loss: 0.2461\n",
      "Epoch [2/3], Step [114/316], Loss: 0.4285\n",
      "Epoch [2/3], Step [115/316], Loss: 0.3735\n",
      "Epoch [2/3], Step [116/316], Loss: 0.4631\n",
      "Epoch [2/3], Step [117/316], Loss: 0.5324\n",
      "Epoch [2/3], Step [118/316], Loss: 0.5071\n",
      "Epoch [2/3], Step [119/316], Loss: 0.4118\n",
      "Epoch [2/3], Step [120/316], Loss: 0.4370\n",
      "Epoch [2/3], Step [121/316], Loss: 0.3124\n",
      "Epoch [2/3], Step [122/316], Loss: 0.2658\n",
      "Epoch [2/3], Step [123/316], Loss: 0.5764\n",
      "Epoch [2/3], Step [124/316], Loss: 0.7220\n",
      "Epoch [2/3], Step [125/316], Loss: 0.3807\n",
      "Epoch [2/3], Step [126/316], Loss: 0.8720\n",
      "Epoch [2/3], Step [127/316], Loss: 0.2020\n",
      "Epoch [2/3], Step [128/316], Loss: 0.3428\n",
      "Epoch [2/3], Step [129/316], Loss: 0.5424\n",
      "Epoch [2/3], Step [130/316], Loss: 0.3659\n",
      "Epoch [2/3], Step [131/316], Loss: 0.3014\n",
      "Epoch [2/3], Step [132/316], Loss: 0.4344\n",
      "Epoch [2/3], Step [133/316], Loss: 0.3001\n",
      "Epoch [2/3], Step [134/316], Loss: 1.0171\n",
      "Epoch [2/3], Step [135/316], Loss: 0.5189\n",
      "Epoch [2/3], Step [136/316], Loss: 0.7084\n",
      "Epoch [2/3], Step [137/316], Loss: 0.3937\n",
      "Epoch [2/3], Step [138/316], Loss: 0.3562\n",
      "Epoch [2/3], Step [139/316], Loss: 0.8174\n",
      "Epoch [2/3], Step [140/316], Loss: 0.5099\n",
      "Epoch [2/3], Step [141/316], Loss: 0.4049\n",
      "Epoch [2/3], Step [142/316], Loss: 0.3886\n",
      "Epoch [2/3], Step [143/316], Loss: 0.2961\n",
      "Epoch [2/3], Step [144/316], Loss: 0.7708\n",
      "Epoch [2/3], Step [145/316], Loss: 0.4966\n",
      "Epoch [2/3], Step [146/316], Loss: 0.2110\n",
      "Epoch [2/3], Step [147/316], Loss: 0.7464\n",
      "Epoch [2/3], Step [148/316], Loss: 0.4550\n",
      "Epoch [2/3], Step [149/316], Loss: 0.2444\n",
      "Epoch [2/3], Step [150/316], Loss: 0.5938\n",
      "Epoch [2/3], Step [151/316], Loss: 0.4062\n",
      "Epoch [2/3], Step [152/316], Loss: 0.6352\n",
      "Epoch [2/3], Step [153/316], Loss: 0.5299\n",
      "Epoch [2/3], Step [154/316], Loss: 0.4823\n",
      "Epoch [2/3], Step [155/316], Loss: 0.5462\n",
      "Epoch [2/3], Step [156/316], Loss: 0.6160\n",
      "Epoch [2/3], Step [157/316], Loss: 0.3832\n",
      "Epoch [2/3], Step [158/316], Loss: 0.4975\n",
      "Epoch [2/3], Step [159/316], Loss: 0.4106\n",
      "Epoch [2/3], Step [160/316], Loss: 0.4174\n",
      "Epoch [2/3], Step [161/316], Loss: 0.3963\n",
      "Epoch [2/3], Step [162/316], Loss: 0.5523\n",
      "Epoch [2/3], Step [163/316], Loss: 0.3826\n",
      "Epoch [2/3], Step [164/316], Loss: 0.4371\n",
      "Epoch [2/3], Step [165/316], Loss: 0.7123\n",
      "Epoch [2/3], Step [166/316], Loss: 0.5287\n",
      "Epoch [2/3], Step [167/316], Loss: 0.4447\n",
      "Epoch [2/3], Step [168/316], Loss: 0.5602\n",
      "Epoch [2/3], Step [169/316], Loss: 0.4137\n",
      "Epoch [2/3], Step [170/316], Loss: 0.3088\n",
      "Epoch [2/3], Step [171/316], Loss: 0.2765\n",
      "Epoch [2/3], Step [172/316], Loss: 0.5273\n",
      "Epoch [2/3], Step [173/316], Loss: 0.4107\n",
      "Epoch [2/3], Step [174/316], Loss: 0.5748\n",
      "Epoch [2/3], Step [175/316], Loss: 0.4922\n",
      "Epoch [2/3], Step [176/316], Loss: 0.5732\n",
      "Epoch [2/3], Step [177/316], Loss: 0.4254\n",
      "Epoch [2/3], Step [178/316], Loss: 0.4719\n",
      "Epoch [2/3], Step [179/316], Loss: 0.3422\n",
      "Epoch [2/3], Step [180/316], Loss: 0.2989\n",
      "Epoch [2/3], Step [181/316], Loss: 0.5523\n",
      "Epoch [2/3], Step [182/316], Loss: 0.4725\n",
      "Epoch [2/3], Step [183/316], Loss: 0.3003\n",
      "Epoch [2/3], Step [184/316], Loss: 0.3833\n",
      "Epoch [2/3], Step [185/316], Loss: 0.4015\n",
      "Epoch [2/3], Step [186/316], Loss: 0.2472\n",
      "Epoch [2/3], Step [187/316], Loss: 0.2477\n",
      "Epoch [2/3], Step [188/316], Loss: 0.3793\n",
      "Epoch [2/3], Step [189/316], Loss: 0.5482\n",
      "Epoch [2/3], Step [190/316], Loss: 0.4505\n",
      "Epoch [2/3], Step [191/316], Loss: 0.3970\n",
      "Epoch [2/3], Step [192/316], Loss: 0.2677\n",
      "Epoch [2/3], Step [193/316], Loss: 0.5556\n",
      "Epoch [2/3], Step [194/316], Loss: 0.3115\n",
      "Epoch [2/3], Step [195/316], Loss: 0.4798\n",
      "Epoch [2/3], Step [196/316], Loss: 0.4687\n",
      "Epoch [2/3], Step [197/316], Loss: 0.5550\n",
      "Epoch [2/3], Step [198/316], Loss: 0.3558\n",
      "Epoch [2/3], Step [199/316], Loss: 0.5050\n",
      "Epoch [2/3], Step [200/316], Loss: 0.4531\n",
      "Epoch [2/3], Step [201/316], Loss: 0.5259\n",
      "Epoch [2/3], Step [202/316], Loss: 0.3415\n",
      "Epoch [2/3], Step [203/316], Loss: 0.5620\n",
      "Epoch [2/3], Step [204/316], Loss: 0.4853\n",
      "Epoch [2/3], Step [205/316], Loss: 0.5825\n",
      "Epoch [2/3], Step [206/316], Loss: 0.6346\n",
      "Epoch [2/3], Step [207/316], Loss: 0.2645\n",
      "Epoch [2/3], Step [208/316], Loss: 0.4717\n",
      "Epoch [2/3], Step [209/316], Loss: 0.4223\n",
      "Epoch [2/3], Step [210/316], Loss: 0.4692\n",
      "Epoch [2/3], Step [211/316], Loss: 0.5778\n",
      "Epoch [2/3], Step [212/316], Loss: 0.5910\n",
      "Epoch [2/3], Step [213/316], Loss: 0.3534\n",
      "Epoch [2/3], Step [214/316], Loss: 0.4998\n",
      "Epoch [2/3], Step [215/316], Loss: 0.3935\n",
      "Epoch [2/3], Step [216/316], Loss: 0.5115\n",
      "Epoch [2/3], Step [217/316], Loss: 0.5144\n",
      "Epoch [2/3], Step [218/316], Loss: 0.4256\n",
      "Epoch [2/3], Step [219/316], Loss: 0.4780\n",
      "Epoch [2/3], Step [220/316], Loss: 0.6197\n",
      "Epoch [2/3], Step [221/316], Loss: 0.4068\n",
      "Epoch [2/3], Step [222/316], Loss: 0.4915\n",
      "Epoch [2/3], Step [223/316], Loss: 0.8017\n",
      "Epoch [2/3], Step [224/316], Loss: 0.6674\n",
      "Epoch [2/3], Step [225/316], Loss: 0.4388\n",
      "Epoch [2/3], Step [226/316], Loss: 0.4157\n",
      "Epoch [2/3], Step [227/316], Loss: 0.4134\n",
      "Epoch [2/3], Step [228/316], Loss: 0.4241\n",
      "Epoch [2/3], Step [229/316], Loss: 0.4294\n",
      "Epoch [2/3], Step [230/316], Loss: 0.2490\n",
      "Epoch [2/3], Step [231/316], Loss: 0.4798\n",
      "Epoch [2/3], Step [232/316], Loss: 0.4777\n",
      "Epoch [2/3], Step [233/316], Loss: 0.4766\n",
      "Epoch [2/3], Step [234/316], Loss: 0.4487\n",
      "Epoch [2/3], Step [235/316], Loss: 0.5311\n",
      "Epoch [2/3], Step [236/316], Loss: 0.3858\n",
      "Epoch [2/3], Step [237/316], Loss: 0.3865\n",
      "Epoch [2/3], Step [238/316], Loss: 0.4777\n",
      "Epoch [2/3], Step [239/316], Loss: 0.5423\n",
      "Epoch [2/3], Step [240/316], Loss: 0.4411\n",
      "Epoch [2/3], Step [241/316], Loss: 0.5555\n",
      "Epoch [2/3], Step [242/316], Loss: 0.3360\n",
      "Epoch [2/3], Step [243/316], Loss: 0.6633\n",
      "Epoch [2/3], Step [244/316], Loss: 0.3760\n",
      "Epoch [2/3], Step [245/316], Loss: 0.2468\n",
      "Epoch [2/3], Step [246/316], Loss: 0.5922\n",
      "Epoch [2/3], Step [247/316], Loss: 0.2791\n",
      "Epoch [2/3], Step [248/316], Loss: 0.2801\n",
      "Epoch [2/3], Step [249/316], Loss: 0.5099\n",
      "Epoch [2/3], Step [250/316], Loss: 0.6541\n",
      "Epoch [2/3], Step [251/316], Loss: 0.4930\n",
      "Epoch [2/3], Step [252/316], Loss: 0.5510\n",
      "Epoch [2/3], Step [253/316], Loss: 0.3950\n",
      "Epoch [2/3], Step [254/316], Loss: 0.5601\n",
      "Epoch [2/3], Step [255/316], Loss: 0.4186\n",
      "Epoch [2/3], Step [256/316], Loss: 0.2952\n",
      "Epoch [2/3], Step [257/316], Loss: 0.2309\n",
      "Epoch [2/3], Step [258/316], Loss: 0.2717\n",
      "Epoch [2/3], Step [259/316], Loss: 1.0071\n",
      "Epoch [2/3], Step [260/316], Loss: 0.5863\n",
      "Epoch [2/3], Step [261/316], Loss: 0.4728\n",
      "Epoch [2/3], Step [262/316], Loss: 0.5357\n",
      "Epoch [2/3], Step [263/316], Loss: 0.4833\n",
      "Epoch [2/3], Step [264/316], Loss: 0.6160\n",
      "Epoch [2/3], Step [265/316], Loss: 0.4678\n",
      "Epoch [2/3], Step [266/316], Loss: 0.7189\n",
      "Epoch [2/3], Step [267/316], Loss: 0.2785\n",
      "Epoch [2/3], Step [268/316], Loss: 0.6814\n",
      "Epoch [2/3], Step [269/316], Loss: 0.2121\n",
      "Epoch [2/3], Step [270/316], Loss: 0.5622\n",
      "Epoch [2/3], Step [271/316], Loss: 0.3306\n",
      "Epoch [2/3], Step [272/316], Loss: 0.6285\n",
      "Epoch [2/3], Step [273/316], Loss: 0.5035\n",
      "Epoch [2/3], Step [274/316], Loss: 0.6308\n",
      "Epoch [2/3], Step [275/316], Loss: 0.3232\n",
      "Epoch [2/3], Step [276/316], Loss: 0.4840\n",
      "Epoch [2/3], Step [277/316], Loss: 0.3678\n",
      "Epoch [2/3], Step [278/316], Loss: 0.5092\n",
      "Epoch [2/3], Step [279/316], Loss: 0.4280\n",
      "Epoch [2/3], Step [280/316], Loss: 0.5213\n",
      "Epoch [2/3], Step [281/316], Loss: 0.3260\n",
      "Epoch [2/3], Step [282/316], Loss: 0.4073\n",
      "Epoch [2/3], Step [283/316], Loss: 0.4519\n",
      "Epoch [2/3], Step [284/316], Loss: 0.3804\n",
      "Epoch [2/3], Step [285/316], Loss: 0.2207\n",
      "Epoch [2/3], Step [286/316], Loss: 0.4419\n",
      "Epoch [2/3], Step [287/316], Loss: 0.3591\n",
      "Epoch [2/3], Step [288/316], Loss: 0.2015\n",
      "Epoch [2/3], Step [289/316], Loss: 0.5682\n",
      "Epoch [2/3], Step [290/316], Loss: 0.5405\n",
      "Epoch [2/3], Step [291/316], Loss: 0.3609\n",
      "Epoch [2/3], Step [292/316], Loss: 0.4152\n",
      "Epoch [2/3], Step [293/316], Loss: 0.1733\n",
      "Epoch [2/3], Step [294/316], Loss: 0.5485\n",
      "Epoch [2/3], Step [295/316], Loss: 0.3384\n",
      "Epoch [2/3], Step [296/316], Loss: 0.3943\n",
      "Epoch [2/3], Step [297/316], Loss: 0.6369\n",
      "Epoch [2/3], Step [298/316], Loss: 0.5783\n",
      "Epoch [2/3], Step [299/316], Loss: 0.4346\n",
      "Epoch [2/3], Step [300/316], Loss: 0.3537\n",
      "Epoch [2/3], Step [301/316], Loss: 0.6146\n",
      "Epoch [2/3], Step [302/316], Loss: 0.2386\n",
      "Epoch [2/3], Step [303/316], Loss: 0.3379\n",
      "Epoch [2/3], Step [304/316], Loss: 0.1543\n",
      "Epoch [2/3], Step [305/316], Loss: 0.4072\n",
      "Epoch [2/3], Step [306/316], Loss: 0.3917\n",
      "Epoch [2/3], Step [307/316], Loss: 0.2932\n",
      "Epoch [2/3], Step [308/316], Loss: 0.3085\n",
      "Epoch [2/3], Step [309/316], Loss: 0.4874\n",
      "Epoch [2/3], Step [310/316], Loss: 0.3087\n",
      "Epoch [2/3], Step [311/316], Loss: 0.5943\n",
      "Epoch [2/3], Step [312/316], Loss: 0.3748\n",
      "Epoch [2/3], Step [313/316], Loss: 0.3064\n",
      "Epoch [2/3], Step [314/316], Loss: 0.5187\n",
      "Epoch [2/3], Step [315/316], Loss: 0.3126\n",
      "Epoch [2/3], Step [316/316], Loss: 0.5004\n",
      "Epoch [2/3], Train Loss: 0.4607\n",
      "Epoch [2/3], Validation Accuracy: 0.8211\n",
      "Epoch [3/3], Step [1/316], Loss: 0.3646\n",
      "Epoch [3/3], Step [2/316], Loss: 0.1949\n",
      "Epoch [3/3], Step [3/316], Loss: 0.2084\n",
      "Epoch [3/3], Step [4/316], Loss: 0.2572\n",
      "Epoch [3/3], Step [5/316], Loss: 0.5157\n",
      "Epoch [3/3], Step [6/316], Loss: 0.1916\n",
      "Epoch [3/3], Step [7/316], Loss: 0.3072\n",
      "Epoch [3/3], Step [8/316], Loss: 0.4001\n",
      "Epoch [3/3], Step [9/316], Loss: 0.3639\n",
      "Epoch [3/3], Step [10/316], Loss: 0.3669\n",
      "Epoch [3/3], Step [11/316], Loss: 0.2739\n",
      "Epoch [3/3], Step [12/316], Loss: 0.2687\n",
      "Epoch [3/3], Step [13/316], Loss: 0.3911\n",
      "Epoch [3/3], Step [14/316], Loss: 0.7098\n",
      "Epoch [3/3], Step [15/316], Loss: 0.2919\n",
      "Epoch [3/3], Step [16/316], Loss: 0.3224\n",
      "Epoch [3/3], Step [17/316], Loss: 0.2375\n",
      "Epoch [3/3], Step [18/316], Loss: 0.3082\n",
      "Epoch [3/3], Step [19/316], Loss: 0.2052\n",
      "Epoch [3/3], Step [20/316], Loss: 0.3439\n",
      "Epoch [3/3], Step [21/316], Loss: 0.1972\n",
      "Epoch [3/3], Step [22/316], Loss: 0.3023\n",
      "Epoch [3/3], Step [23/316], Loss: 0.2549\n",
      "Epoch [3/3], Step [24/316], Loss: 0.4302\n",
      "Epoch [3/3], Step [25/316], Loss: 0.5275\n",
      "Epoch [3/3], Step [26/316], Loss: 0.4834\n",
      "Epoch [3/3], Step [27/316], Loss: 0.2811\n",
      "Epoch [3/3], Step [28/316], Loss: 0.2635\n",
      "Epoch [3/3], Step [29/316], Loss: 0.4347\n",
      "Epoch [3/3], Step [30/316], Loss: 0.2464\n",
      "Epoch [3/3], Step [31/316], Loss: 0.3559\n",
      "Epoch [3/3], Step [32/316], Loss: 0.4529\n",
      "Epoch [3/3], Step [33/316], Loss: 0.3086\n",
      "Epoch [3/3], Step [34/316], Loss: 0.3127\n",
      "Epoch [3/3], Step [35/316], Loss: 0.3813\n",
      "Epoch [3/3], Step [36/316], Loss: 0.2663\n",
      "Epoch [3/3], Step [37/316], Loss: 0.3495\n",
      "Epoch [3/3], Step [38/316], Loss: 0.3140\n",
      "Epoch [3/3], Step [39/316], Loss: 0.2853\n",
      "Epoch [3/3], Step [40/316], Loss: 0.1597\n",
      "Epoch [3/3], Step [41/316], Loss: 0.6043\n",
      "Epoch [3/3], Step [42/316], Loss: 0.3322\n",
      "Epoch [3/3], Step [43/316], Loss: 0.2793\n",
      "Epoch [3/3], Step [44/316], Loss: 0.4437\n",
      "Epoch [3/3], Step [45/316], Loss: 0.2660\n",
      "Epoch [3/3], Step [46/316], Loss: 0.1303\n",
      "Epoch [3/3], Step [47/316], Loss: 0.3171\n",
      "Epoch [3/3], Step [48/316], Loss: 0.3296\n",
      "Epoch [3/3], Step [49/316], Loss: 0.2445\n",
      "Epoch [3/3], Step [50/316], Loss: 0.3564\n",
      "Epoch [3/3], Step [51/316], Loss: 0.1567\n",
      "Epoch [3/3], Step [52/316], Loss: 0.3564\n",
      "Epoch [3/3], Step [53/316], Loss: 0.5116\n",
      "Epoch [3/3], Step [54/316], Loss: 0.2182\n",
      "Epoch [3/3], Step [55/316], Loss: 0.1658\n",
      "Epoch [3/3], Step [56/316], Loss: 0.1706\n",
      "Epoch [3/3], Step [57/316], Loss: 0.6491\n",
      "Epoch [3/3], Step [58/316], Loss: 0.3965\n",
      "Epoch [3/3], Step [59/316], Loss: 0.3974\n",
      "Epoch [3/3], Step [60/316], Loss: 0.2141\n",
      "Epoch [3/3], Step [61/316], Loss: 0.2473\n",
      "Epoch [3/3], Step [62/316], Loss: 0.3536\n",
      "Epoch [3/3], Step [63/316], Loss: 0.2652\n",
      "Epoch [3/3], Step [64/316], Loss: 0.5847\n",
      "Epoch [3/3], Step [65/316], Loss: 0.2497\n",
      "Epoch [3/3], Step [66/316], Loss: 0.1252\n",
      "Epoch [3/3], Step [67/316], Loss: 0.4101\n",
      "Epoch [3/3], Step [68/316], Loss: 0.2755\n",
      "Epoch [3/3], Step [69/316], Loss: 0.3333\n",
      "Epoch [3/3], Step [70/316], Loss: 0.1583\n",
      "Epoch [3/3], Step [71/316], Loss: 0.4684\n",
      "Epoch [3/3], Step [72/316], Loss: 0.4705\n",
      "Epoch [3/3], Step [73/316], Loss: 0.3315\n",
      "Epoch [3/3], Step [74/316], Loss: 0.2153\n",
      "Epoch [3/3], Step [75/316], Loss: 0.2086\n",
      "Epoch [3/3], Step [76/316], Loss: 0.6929\n",
      "Epoch [3/3], Step [77/316], Loss: 0.3113\n",
      "Epoch [3/3], Step [78/316], Loss: 0.2958\n",
      "Epoch [3/3], Step [79/316], Loss: 0.3464\n",
      "Epoch [3/3], Step [80/316], Loss: 0.2371\n",
      "Epoch [3/3], Step [81/316], Loss: 0.3413\n",
      "Epoch [3/3], Step [82/316], Loss: 0.1610\n",
      "Epoch [3/3], Step [83/316], Loss: 0.6500\n",
      "Epoch [3/3], Step [84/316], Loss: 0.3618\n",
      "Epoch [3/3], Step [85/316], Loss: 0.3878\n",
      "Epoch [3/3], Step [86/316], Loss: 0.4753\n",
      "Epoch [3/3], Step [87/316], Loss: 0.2357\n",
      "Epoch [3/3], Step [88/316], Loss: 0.2968\n",
      "Epoch [3/3], Step [89/316], Loss: 0.2884\n",
      "Epoch [3/3], Step [90/316], Loss: 0.3648\n",
      "Epoch [3/3], Step [91/316], Loss: 0.4105\n",
      "Epoch [3/3], Step [92/316], Loss: 0.3678\n",
      "Epoch [3/3], Step [93/316], Loss: 0.2120\n",
      "Epoch [3/3], Step [94/316], Loss: 0.3676\n",
      "Epoch [3/3], Step [95/316], Loss: 0.3504\n",
      "Epoch [3/3], Step [96/316], Loss: 0.1945\n",
      "Epoch [3/3], Step [97/316], Loss: 0.3078\n",
      "Epoch [3/3], Step [98/316], Loss: 0.2166\n",
      "Epoch [3/3], Step [99/316], Loss: 0.4378\n",
      "Epoch [3/3], Step [100/316], Loss: 0.3568\n",
      "Epoch [3/3], Step [101/316], Loss: 0.1595\n",
      "Epoch [3/3], Step [102/316], Loss: 0.3693\n",
      "Epoch [3/3], Step [103/316], Loss: 0.2180\n",
      "Epoch [3/3], Step [104/316], Loss: 0.3351\n",
      "Epoch [3/3], Step [105/316], Loss: 0.3870\n",
      "Epoch [3/3], Step [106/316], Loss: 0.1766\n",
      "Epoch [3/3], Step [107/316], Loss: 0.1552\n",
      "Epoch [3/3], Step [108/316], Loss: 0.2693\n",
      "Epoch [3/3], Step [109/316], Loss: 0.4030\n",
      "Epoch [3/3], Step [110/316], Loss: 0.2403\n",
      "Epoch [3/3], Step [111/316], Loss: 0.3351\n",
      "Epoch [3/3], Step [112/316], Loss: 0.4631\n",
      "Epoch [3/3], Step [113/316], Loss: 0.5193\n",
      "Epoch [3/3], Step [114/316], Loss: 0.1325\n",
      "Epoch [3/3], Step [115/316], Loss: 0.4733\n",
      "Epoch [3/3], Step [116/316], Loss: 0.3979\n",
      "Epoch [3/3], Step [117/316], Loss: 0.1686\n",
      "Epoch [3/3], Step [118/316], Loss: 0.5194\n",
      "Epoch [3/3], Step [119/316], Loss: 0.2859\n",
      "Epoch [3/3], Step [120/316], Loss: 0.3124\n",
      "Epoch [3/3], Step [121/316], Loss: 0.5561\n",
      "Epoch [3/3], Step [122/316], Loss: 0.4608\n",
      "Epoch [3/3], Step [123/316], Loss: 0.3767\n",
      "Epoch [3/3], Step [124/316], Loss: 0.1985\n",
      "Epoch [3/3], Step [125/316], Loss: 0.2867\n",
      "Epoch [3/3], Step [126/316], Loss: 0.3666\n",
      "Epoch [3/3], Step [127/316], Loss: 0.6849\n",
      "Epoch [3/3], Step [128/316], Loss: 0.5241\n",
      "Epoch [3/3], Step [129/316], Loss: 0.5277\n",
      "Epoch [3/3], Step [130/316], Loss: 0.4010\n",
      "Epoch [3/3], Step [131/316], Loss: 0.6229\n",
      "Epoch [3/3], Step [132/316], Loss: 0.3713\n",
      "Epoch [3/3], Step [133/316], Loss: 0.4325\n",
      "Epoch [3/3], Step [134/316], Loss: 0.2970\n",
      "Epoch [3/3], Step [135/316], Loss: 0.3885\n",
      "Epoch [3/3], Step [136/316], Loss: 0.4839\n",
      "Epoch [3/3], Step [137/316], Loss: 0.6537\n",
      "Epoch [3/3], Step [138/316], Loss: 0.2550\n",
      "Epoch [3/3], Step [139/316], Loss: 0.2355\n",
      "Epoch [3/3], Step [140/316], Loss: 0.2552\n",
      "Epoch [3/3], Step [141/316], Loss: 0.3634\n",
      "Epoch [3/3], Step [142/316], Loss: 0.4719\n",
      "Epoch [3/3], Step [143/316], Loss: 0.3325\n",
      "Epoch [3/3], Step [144/316], Loss: 0.6245\n",
      "Epoch [3/3], Step [145/316], Loss: 0.2791\n",
      "Epoch [3/3], Step [146/316], Loss: 0.3515\n",
      "Epoch [3/3], Step [147/316], Loss: 0.3073\n",
      "Epoch [3/3], Step [148/316], Loss: 0.4611\n",
      "Epoch [3/3], Step [149/316], Loss: 0.3340\n",
      "Epoch [3/3], Step [150/316], Loss: 0.3103\n",
      "Epoch [3/3], Step [151/316], Loss: 0.3674\n",
      "Epoch [3/3], Step [152/316], Loss: 0.2414\n",
      "Epoch [3/3], Step [153/316], Loss: 0.4176\n",
      "Epoch [3/3], Step [154/316], Loss: 0.2641\n",
      "Epoch [3/3], Step [155/316], Loss: 0.3435\n",
      "Epoch [3/3], Step [156/316], Loss: 0.2769\n",
      "Epoch [3/3], Step [157/316], Loss: 0.2892\n",
      "Epoch [3/3], Step [158/316], Loss: 0.5447\n",
      "Epoch [3/3], Step [159/316], Loss: 0.1483\n",
      "Epoch [3/3], Step [160/316], Loss: 0.1862\n",
      "Epoch [3/3], Step [161/316], Loss: 0.2772\n",
      "Epoch [3/3], Step [162/316], Loss: 1.3241\n",
      "Epoch [3/3], Step [163/316], Loss: 0.8819\n",
      "Epoch [3/3], Step [164/316], Loss: 0.2891\n",
      "Epoch [3/3], Step [165/316], Loss: 0.7719\n",
      "Epoch [3/3], Step [166/316], Loss: 0.2619\n",
      "Epoch [3/3], Step [167/316], Loss: 0.4385\n",
      "Epoch [3/3], Step [168/316], Loss: 0.5655\n",
      "Epoch [3/3], Step [169/316], Loss: 0.3355\n",
      "Epoch [3/3], Step [170/316], Loss: 0.3601\n",
      "Epoch [3/3], Step [171/316], Loss: 0.4008\n",
      "Epoch [3/3], Step [172/316], Loss: 0.4053\n",
      "Epoch [3/3], Step [173/316], Loss: 0.4805\n",
      "Epoch [3/3], Step [174/316], Loss: 0.3559\n",
      "Epoch [3/3], Step [175/316], Loss: 0.3113\n",
      "Epoch [3/3], Step [176/316], Loss: 0.2679\n",
      "Epoch [3/3], Step [177/316], Loss: 0.3566\n",
      "Epoch [3/3], Step [178/316], Loss: 0.1625\n",
      "Epoch [3/3], Step [179/316], Loss: 0.3472\n",
      "Epoch [3/3], Step [180/316], Loss: 0.3756\n",
      "Epoch [3/3], Step [181/316], Loss: 0.6094\n",
      "Epoch [3/3], Step [182/316], Loss: 0.3645\n",
      "Epoch [3/3], Step [183/316], Loss: 0.2113\n",
      "Epoch [3/3], Step [184/316], Loss: 0.4378\n",
      "Epoch [3/3], Step [185/316], Loss: 0.4331\n",
      "Epoch [3/3], Step [186/316], Loss: 0.4305\n",
      "Epoch [3/3], Step [187/316], Loss: 0.2771\n",
      "Epoch [3/3], Step [188/316], Loss: 0.3307\n",
      "Epoch [3/3], Step [189/316], Loss: 0.6047\n",
      "Epoch [3/3], Step [190/316], Loss: 0.3041\n",
      "Epoch [3/3], Step [191/316], Loss: 0.2613\n",
      "Epoch [3/3], Step [192/316], Loss: 0.3338\n",
      "Epoch [3/3], Step [193/316], Loss: 0.3683\n",
      "Epoch [3/3], Step [194/316], Loss: 0.4697\n",
      "Epoch [3/3], Step [195/316], Loss: 0.3606\n",
      "Epoch [3/3], Step [196/316], Loss: 0.6633\n",
      "Epoch [3/3], Step [197/316], Loss: 0.4013\n",
      "Epoch [3/3], Step [198/316], Loss: 0.3373\n",
      "Epoch [3/3], Step [199/316], Loss: 0.4204\n",
      "Epoch [3/3], Step [200/316], Loss: 0.2855\n",
      "Epoch [3/3], Step [201/316], Loss: 0.3810\n",
      "Epoch [3/3], Step [202/316], Loss: 0.1892\n",
      "Epoch [3/3], Step [203/316], Loss: 0.3400\n",
      "Epoch [3/3], Step [204/316], Loss: 0.3511\n",
      "Epoch [3/3], Step [205/316], Loss: 0.3592\n",
      "Epoch [3/3], Step [206/316], Loss: 0.1791\n",
      "Epoch [3/3], Step [207/316], Loss: 0.3063\n",
      "Epoch [3/3], Step [208/316], Loss: 0.1593\n",
      "Epoch [3/3], Step [209/316], Loss: 0.2362\n",
      "Epoch [3/3], Step [210/316], Loss: 0.2443\n",
      "Epoch [3/3], Step [211/316], Loss: 0.2992\n",
      "Epoch [3/3], Step [212/316], Loss: 0.2869\n",
      "Epoch [3/3], Step [213/316], Loss: 0.3283\n",
      "Epoch [3/3], Step [214/316], Loss: 0.2381\n",
      "Epoch [3/3], Step [215/316], Loss: 0.3006\n",
      "Epoch [3/3], Step [216/316], Loss: 0.1492\n",
      "Epoch [3/3], Step [217/316], Loss: 0.3290\n",
      "Epoch [3/3], Step [218/316], Loss: 0.2663\n",
      "Epoch [3/3], Step [219/316], Loss: 0.3784\n",
      "Epoch [3/3], Step [220/316], Loss: 0.4568\n",
      "Epoch [3/3], Step [221/316], Loss: 0.2549\n",
      "Epoch [3/3], Step [222/316], Loss: 0.3422\n",
      "Epoch [3/3], Step [223/316], Loss: 0.3052\n",
      "Epoch [3/3], Step [224/316], Loss: 0.3042\n",
      "Epoch [3/3], Step [225/316], Loss: 0.1268\n",
      "Epoch [3/3], Step [226/316], Loss: 0.4555\n",
      "Epoch [3/3], Step [227/316], Loss: 0.3768\n",
      "Epoch [3/3], Step [228/316], Loss: 0.3368\n",
      "Epoch [3/3], Step [229/316], Loss: 0.3756\n",
      "Epoch [3/3], Step [230/316], Loss: 0.5223\n",
      "Epoch [3/3], Step [231/316], Loss: 0.3957\n",
      "Epoch [3/3], Step [232/316], Loss: 0.4181\n",
      "Epoch [3/3], Step [233/316], Loss: 0.3478\n",
      "Epoch [3/3], Step [234/316], Loss: 0.4963\n",
      "Epoch [3/3], Step [235/316], Loss: 0.4359\n",
      "Epoch [3/3], Step [236/316], Loss: 0.3503\n",
      "Epoch [3/3], Step [237/316], Loss: 0.3498\n",
      "Epoch [3/3], Step [238/316], Loss: 0.2543\n",
      "Epoch [3/3], Step [239/316], Loss: 0.3914\n",
      "Epoch [3/3], Step [240/316], Loss: 0.4464\n",
      "Epoch [3/3], Step [241/316], Loss: 0.6048\n",
      "Epoch [3/3], Step [242/316], Loss: 0.4017\n",
      "Epoch [3/3], Step [243/316], Loss: 0.4050\n",
      "Epoch [3/3], Step [244/316], Loss: 0.3053\n",
      "Epoch [3/3], Step [245/316], Loss: 0.3810\n",
      "Epoch [3/3], Step [246/316], Loss: 0.2401\n",
      "Epoch [3/3], Step [247/316], Loss: 0.4567\n",
      "Epoch [3/3], Step [248/316], Loss: 0.4726\n",
      "Epoch [3/3], Step [249/316], Loss: 0.1994\n",
      "Epoch [3/3], Step [250/316], Loss: 0.2674\n",
      "Epoch [3/3], Step [251/316], Loss: 0.3236\n",
      "Epoch [3/3], Step [252/316], Loss: 0.2270\n",
      "Epoch [3/3], Step [253/316], Loss: 0.5421\n",
      "Epoch [3/3], Step [254/316], Loss: 0.2333\n",
      "Epoch [3/3], Step [255/316], Loss: 0.3258\n",
      "Epoch [3/3], Step [256/316], Loss: 0.3633\n",
      "Epoch [3/3], Step [257/316], Loss: 0.1578\n",
      "Epoch [3/3], Step [258/316], Loss: 0.3726\n",
      "Epoch [3/3], Step [259/316], Loss: 0.3240\n",
      "Epoch [3/3], Step [260/316], Loss: 0.8309\n",
      "Epoch [3/3], Step [261/316], Loss: 0.2521\n",
      "Epoch [3/3], Step [262/316], Loss: 0.2768\n",
      "Epoch [3/3], Step [263/316], Loss: 0.3587\n",
      "Epoch [3/3], Step [264/316], Loss: 0.2301\n",
      "Epoch [3/3], Step [265/316], Loss: 0.2933\n",
      "Epoch [3/3], Step [266/316], Loss: 0.4484\n",
      "Epoch [3/3], Step [267/316], Loss: 0.2714\n",
      "Epoch [3/3], Step [268/316], Loss: 0.1724\n",
      "Epoch [3/3], Step [269/316], Loss: 0.2248\n",
      "Epoch [3/3], Step [270/316], Loss: 0.3465\n",
      "Epoch [3/3], Step [271/316], Loss: 0.1749\n",
      "Epoch [3/3], Step [272/316], Loss: 0.1681\n",
      "Epoch [3/3], Step [273/316], Loss: 0.4918\n",
      "Epoch [3/3], Step [274/316], Loss: 0.4071\n",
      "Epoch [3/3], Step [275/316], Loss: 0.4339\n",
      "Epoch [3/3], Step [276/316], Loss: 0.3245\n",
      "Epoch [3/3], Step [277/316], Loss: 0.2489\n",
      "Epoch [3/3], Step [278/316], Loss: 0.3871\n",
      "Epoch [3/3], Step [279/316], Loss: 0.3174\n",
      "Epoch [3/3], Step [280/316], Loss: 0.1388\n",
      "Epoch [3/3], Step [281/316], Loss: 0.6469\n",
      "Epoch [3/3], Step [282/316], Loss: 0.4808\n",
      "Epoch [3/3], Step [283/316], Loss: 0.3359\n",
      "Epoch [3/3], Step [284/316], Loss: 0.2406\n",
      "Epoch [3/3], Step [285/316], Loss: 0.5112\n",
      "Epoch [3/3], Step [286/316], Loss: 0.4100\n",
      "Epoch [3/3], Step [287/316], Loss: 0.3239\n",
      "Epoch [3/3], Step [288/316], Loss: 0.3962\n",
      "Epoch [3/3], Step [289/316], Loss: 0.4553\n",
      "Epoch [3/3], Step [290/316], Loss: 0.1439\n",
      "Epoch [3/3], Step [291/316], Loss: 0.4901\n",
      "Epoch [3/3], Step [292/316], Loss: 0.3530\n",
      "Epoch [3/3], Step [293/316], Loss: 0.4735\n",
      "Epoch [3/3], Step [294/316], Loss: 0.5002\n",
      "Epoch [3/3], Step [295/316], Loss: 0.3089\n",
      "Epoch [3/3], Step [296/316], Loss: 0.2163\n",
      "Epoch [3/3], Step [297/316], Loss: 0.4752\n",
      "Epoch [3/3], Step [298/316], Loss: 0.2173\n",
      "Epoch [3/3], Step [299/316], Loss: 0.3713\n",
      "Epoch [3/3], Step [300/316], Loss: 0.4243\n",
      "Epoch [3/3], Step [301/316], Loss: 0.4730\n",
      "Epoch [3/3], Step [302/316], Loss: 0.4007\n",
      "Epoch [3/3], Step [303/316], Loss: 0.4022\n",
      "Epoch [3/3], Step [304/316], Loss: 0.2053\n",
      "Epoch [3/3], Step [305/316], Loss: 0.1631\n",
      "Epoch [3/3], Step [306/316], Loss: 0.3982\n",
      "Epoch [3/3], Step [307/316], Loss: 0.4790\n",
      "Epoch [3/3], Step [308/316], Loss: 0.5010\n",
      "Epoch [3/3], Step [309/316], Loss: 0.3970\n",
      "Epoch [3/3], Step [310/316], Loss: 0.2009\n",
      "Epoch [3/3], Step [311/316], Loss: 0.1841\n",
      "Epoch [3/3], Step [312/316], Loss: 0.2675\n",
      "Epoch [3/3], Step [313/316], Loss: 0.3225\n",
      "Epoch [3/3], Step [314/316], Loss: 0.2242\n",
      "Epoch [3/3], Step [315/316], Loss: 0.4938\n",
      "Epoch [3/3], Step [316/316], Loss: 0.2917\n",
      "Epoch [3/3], Train Loss: 0.3518\n",
      "Epoch [3/3], Validation Accuracy: 0.8424\n",
      "Training complete!\n",
      "CPU times: user 1h 15min 14s, sys: 20min 29s, total: 1h 35min 43s\n",
      "Wall time: 40min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# DONE: implement training loop\n",
    "\n",
    "# you can play with different number of epochs and check the model's performance!\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for iteration, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{iteration+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for iteration, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            total_correct += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "         accuracy = total_correct / total_samples\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7No7YmU-RA30",
    "outputId": "455c4f86-1640-46f4-dc74-13abeb3714dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8347\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate the model based on test_loader\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        total_correct += (predicted_labels == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b765P0CFH2g7"
   },
   "source": [
    "## SHAP Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5BYGaVB6BZ9"
   },
   "source": [
    "Now, we have a decent model for toxic speech detection. However, sometimes it can be not so clear why some sample is considered toxic or not. We can try to explaine the model's decision! For this, we utilize [SHAP](https://shap.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E47PHRX_ILWM",
    "outputId": "0aeb2976-07c0-42ae-9a8b-f657ea572970"
   },
   "outputs": [],
   "source": [
    "# example of module installation\n",
    "\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa-tNTrqE6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.0\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append('/content/drive/MyDrive/tum/tum_exercises/Ex6_XAI_original/src')\n",
    "#sys.path.append('/content/drive/MyDrive/tum/tum_exercises/Ex6_XAI_original/src/evaluation.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 43
    },
    "id": "bSbEY_YbIDk1",
    "outputId": "f7c3e7bf-9f94-4aac-a6b6-314a1255de3d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from src.explainability import shap_explain_text\n",
    "\n",
    "# Initializes JavaScript to visualize plots generated by Shap\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd_efPnkH52-"
   },
   "source": [
    "### Let's inspect our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCHprAyb82kW"
   },
   "source": [
    "#### Easy initialization and exploration of a cherry-picked sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itGpWSKhQKfQ"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hU5262G_QGvH",
    "outputId": "fcdc86a1-24aa-400a-8e16-65391e67cf31"
   },
   "outputs": [],
   "source": [
    "# We need to create a text-classification pipeline to input it to the explainer\n",
    "\n",
    "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKfwxfFuH4q0"
   },
   "outputs": [],
   "source": [
    "# This is the example how to define Explainer from SHAP\n",
    "\n",
    "explainer = shap.Explainer(pred)\n",
    "text = ['only men can have higher education']\n",
    "shap_values = explainer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "JscUFy03QN0M",
    "outputId": "19cb4505-933d-4411-8221-979ca9792feb"
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqDvXB1r-Rfs"
   },
   "source": [
    "In addition to slicing, Explanation objects also support a set of reducing methods. Here we use the ``.mean(0)`` to take the average impact of all words towards the “sexism” (``idx=2``) class. Note that here we are also averaging over three examples, to get a better summary you would want to use a larger portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "_MvrIJuV99uB",
    "outputId": "78a7fdc2-1f87-4c4a-f361-ecf465029d8b"
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[:, :, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwfpKxwsH8hh"
   },
   "source": [
    "### We can also work with already fine-tuned for sexism classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5mtRYtV6rup"
   },
   "source": [
    "There are already different models fine-tuned for toxic or sexism speech detection. You can load, for instance, our model [bertweet-sexism](https://huggingface.co/tum-nlp/bertweet-sexism) and try to explaine it as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOSEbXcQJkq-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmJebLvrIC0f",
    "outputId": "bc2533af-94c0-4844-ab3b-a2bf8cb86c5f"
   },
   "outputs": [],
   "source": [
    "# TODO: define tokenizer and model for bertweet-sexism instance.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDzaWsMS7PI4"
   },
   "source": [
    "Here a small example on cherry-picked sentence to explaine the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH1cnr_NJbgT"
   },
   "outputs": [],
   "source": [
    "# TODO: inspect the model on the sample!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKmUTI8Y7xEO"
   },
   "source": [
    "**TODO: how can we understand if some model is better or not?**\n",
    "You can pick the misclassified samples for each model and compare the explanations. Can you explaine why the model did mistakes? Are these mistakes the same or not? Try out other ways how to use SHAP [here](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Emotion%20classification%20multiclass%20example.html). Maybe, other ways of explanations can be useful more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqFGfjAr7MQV"
   },
   "source": [
    "**TODO: Write your observations here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdJ8LeQY8JqY"
   },
   "source": [
    "**Concluding questions:**\n",
    "* Do the hate scores perform as expected in our model?\n",
    "* If not, can you come up with a possible explanation for that even though the models with social scores performed better?\n",
    "* What does that tell you about applicability of neural networks and their trustworthiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0J-mFEB8PqY"
   },
   "source": [
    "**TODO: Write your answers here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoqau73i7uE7"
   },
   "source": [
    "### <center> Thank you for participating in Social Computing/Social Gaming 2023. </center>\n",
    "### <center> Good luck with the exams! </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eknaChfd1PTi"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Waseem, Z., & Hovy, D. (2016). Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter. In Proceedings of the naacl student research workshop (pp. 88-93).\n",
    "<br> [2] [HuggingFace Tutorial](https://huggingface.co/learn/nlp-course/chapter1/1)\n",
    "<br> [3] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "<br> [4] https://christophm.github.io/interpretable-ml-book/shapley.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QylJgYF15Qb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0367c48ac9f248218a054aca2f4fc247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0374cca38d90444696af2b524c1226df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e680ef3aa5be4a489e16a08a8971da4c",
       "IPY_MODEL_460f53bac5604ff2bce7b9725a488211",
       "IPY_MODEL_daaa115de5c54fdbb00ff23e5da4dddc"
      ],
      "layout": "IPY_MODEL_3c3ec381ed1f499b8919b58b4a19cbea"
     }
    },
    "0867b040cc1f43b09a574f5a65a589fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e696f1f3fe4fa893fcd72e5f2a9d3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1012bc04675458aa4d7e1b440072e43",
      "placeholder": "​",
      "style": "IPY_MODEL_0af093b726be410e8ee82d6de450c0af",
      "value": " 483/483 [00:00&lt;00:00, 8.73kB/s]"
     }
    },
    "0af093b726be410e8ee82d6de450c0af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e3a3699dd6b4cd7b1ef055eb12942a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33e752204a924671956108223cbd6c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_973c4e60b05445aea213b5917fa235ae",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7839b4f86d7e4a20bd3dea4576b68980",
      "value": 483
     }
    },
    "3463b61bbc534af4861d156df5b5dd59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e3a3699dd6b4cd7b1ef055eb12942a6",
      "placeholder": "​",
      "style": "IPY_MODEL_83b78202555d4f029c221b6370a4b4ca",
      "value": " 268M/268M [00:00&lt;00:00, 315MB/s]"
     }
    },
    "38cdf8fa7ca94eaeb60cf119f266287f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38dc92e9f28a4b8a9bb4a6b3dc69e665": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3ec381ed1f499b8919b58b4a19cbea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e4bc097dfec4a84877635315cfc8c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42d3b12688634ea0b14e96bad97680ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "460f53bac5604ff2bce7b9725a488211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60ba0c3288ac4d6c92b4a48755c3382a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e4bc097dfec4a84877635315cfc8c52",
      "value": 231508
     }
    },
    "5e4e67ad32b848fdbf3302806ac9e5b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60328ca17fbc494ea138c1e119d786ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60ba0c3288ac4d6c92b4a48755c3382a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6306ea4bb3c3496da06a6bde10a01c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6fdf320928440088573398fee52539e",
       "IPY_MODEL_33e752204a924671956108223cbd6c0c",
       "IPY_MODEL_08e696f1f3fe4fa893fcd72e5f2a9d3c"
      ],
      "layout": "IPY_MODEL_60328ca17fbc494ea138c1e119d786ee"
     }
    },
    "6318439f0ee74d759f29da84767eb9b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e8c9d3c5ab341cea836b614e9c8724e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e940ab7ba71498387b9ca704bb32d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bc00d075ea44470b7a6ab06f4f5d71b",
      "placeholder": "​",
      "style": "IPY_MODEL_95af74c1855941d592b7e3b1df973741",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.00kB/s]"
     }
    },
    "768fc0d1ceea4ffaa9cbab0ebbcb66b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7839b4f86d7e4a20bd3dea4576b68980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bc00d075ea44470b7a6ab06f4f5d71b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f78e904c343457282cd1ed660593e63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83b78202555d4f029c221b6370a4b4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d7c6352e9244a13816544b547a1295e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "942a0b515ddc4b95b9733282d87c8e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95af74c1855941d592b7e3b1df973741": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "973c4e60b05445aea213b5917fa235ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab18bbdffa844955a32a844a770a281a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af1a87e298e548d5a661027cb61130c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c90d2a3614674d238b41c376b830b002",
       "IPY_MODEL_b5f08dc0567c4bf388f202fe9140deb4",
       "IPY_MODEL_6e940ab7ba71498387b9ca704bb32d33"
      ],
      "layout": "IPY_MODEL_ee50296dc27e41e5ac60c01531c9c2ed"
     }
    },
    "b5f08dc0567c4bf388f202fe9140deb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e8c9d3c5ab341cea836b614e9c8724e",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38cdf8fa7ca94eaeb60cf119f266287f",
      "value": 28
     }
    },
    "b6fdf320928440088573398fee52539e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0867b040cc1f43b09a574f5a65a589fb",
      "placeholder": "​",
      "style": "IPY_MODEL_5e4e67ad32b848fdbf3302806ac9e5b5",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c0ef2f3cb83f4bffb725a96357aef899": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d7c6352e9244a13816544b547a1295e",
      "placeholder": "​",
      "style": "IPY_MODEL_942a0b515ddc4b95b9733282d87c8e48",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "c90d2a3614674d238b41c376b830b002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38dc92e9f28a4b8a9bb4a6b3dc69e665",
      "placeholder": "​",
      "style": "IPY_MODEL_ab18bbdffa844955a32a844a770a281a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "cb2586a6393d4dc9a2d37f178a84a0e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce2ad9f7e1e34124a3964b5cf191c5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0ef2f3cb83f4bffb725a96357aef899",
       "IPY_MODEL_d8cd8f2427884a79b4464ff3dfe919ab",
       "IPY_MODEL_3463b61bbc534af4861d156df5b5dd59"
      ],
      "layout": "IPY_MODEL_42d3b12688634ea0b14e96bad97680ea"
     }
    },
    "d1012bc04675458aa4d7e1b440072e43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8cd8f2427884a79b4464ff3dfe919ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_768fc0d1ceea4ffaa9cbab0ebbcb66b2",
      "max": 267954768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6318439f0ee74d759f29da84767eb9b2",
      "value": 267954768
     }
    },
    "daaa115de5c54fdbb00ff23e5da4dddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc0e30211ddb4876a94fab4a9060c6a0",
      "placeholder": "​",
      "style": "IPY_MODEL_cb2586a6393d4dc9a2d37f178a84a0e2",
      "value": " 232k/232k [00:00&lt;00:00, 3.40MB/s]"
     }
    },
    "e680ef3aa5be4a489e16a08a8971da4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f78e904c343457282cd1ed660593e63",
      "placeholder": "​",
      "style": "IPY_MODEL_0367c48ac9f248218a054aca2f4fc247",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "ee50296dc27e41e5ac60c01531c9c2ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc0e30211ddb4876a94fab4a9060c6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
