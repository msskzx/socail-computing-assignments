{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2023\n",
    "# Exercise 3 - Finding Groups: Clustering Methods\n",
    "\n",
    "Clustering aims at grouping data-points according to certain characteristics of interest. The groupings at maximizing intra-cluster cohesion and inter-cluster de-cohesion. In this exercise, we will take a look at Louvain clustering and a comparison between K-means and Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Louvain Clustering \n",
    "In this section of the exercise, you will look at a clustering method which is an alternative to the Newmann Girvan Method from the lecture, the Louvain algorithm. It is an efficient algorithm for computing graph clusterings. Your task will be to implement it.\n",
    "\n",
    "### Louvain Algorithm\n",
    "The central goal of this algorithm is to sequentially merge nodes into clusters based on modularity as a local optimality criterion. Even though the algorithm is \"greedy\" and not guaranteed to find the best clustering, it gives quite good results for detecting communities and groups. \n",
    "\n",
    "We start with singletons (each node in its own cluster) and pick a node $i$. For each friend $j$ of $i$, we calculate Modularity values that result from adding $i$ to the singleton cluster $j$ (forming a new 2-cluster) or that result from adding $i$ to the non-singleton cluster that contains $j$. We merge $i$ with that $j$ for which the gain in Modularity is highest. We do this sequentially for all nodes $i$ in the graph. \n",
    "\n",
    "Then, each new cluster is replaced by a new node representing the cluster. Edges within the cluster are translated into self-edges of that new cluster node. We can then go back to the first step and iterate. \n",
    "\n",
    "Modularity for a graph with adjacency matrix $A_{ij}$ is by the difference between the number (or added weight) of intra-cluster edges in the actual graph $G(V,E)$ and  the number (or added weight) of intra-cluster edges in a random version of the graph where we keep the degrees (or added edge weights) of each node as in the original graph: \n",
    "\n",
    "$$Q = \\frac{1}{2m}\\sum_{i,j\\in V}[A_{ij}-\\frac{k_ik_j}{2m}]\\delta(c_i, c_j)$$\n",
    "\n",
    "where $m=1/2 \\sum_{i,j\\in V}A_{ij}$ and $c_n$ is the id of the cluster that node $n$ is in and $\\delta(x,y)=1$ if $x=y$ and $\\delta(x,y)=0$ else. \n",
    "\n",
    "We may as well use the following simplified [formula](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html) [1]:<br>\n",
    "$$ Q = \\sum_{c=1}^{n} [\\frac{L_c}{m} - (\\frac{k_c}{2m})^2] $$\n",
    "with $L_c$ being the number of intra-community links for community c, and $k_c$ being the sum of degrees of the nodes in community c.\n",
    "\n",
    "The algorithm's implementation thus proceeds as follows:\n",
    "* (1) For each node $i$: For each neighbour $j$: Calculate modularity values for possible merges of $i$ with the cluster containing $j$. Add $i$ to the cluster with highest resulting modularity gain (if no gain is possible, $i$ stays as it is)\n",
    "* (2) Merge each cluster into a new node representing the cluster. \n",
    "* Repeat (1) + (2) until no improvement can be made\n",
    "\n",
    "**Write a Python program that computes the graph clustering with the Louvain method by completing the tasks below.** The program's input is a NetworkX Graph object. The output should be the value of the optimal modularity value found and the corresponding Graph object representing the best clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Modularity\n",
    "In order to compare different clusterings later, the first thing to do is **implementing a function for the modularity calculation** by means of the formula given above. \n",
    "\n",
    "In addition to that we want to calculate the modularity gain when analyzing possible merges for a node $i$ into a cluster $c$. We get the following expression:\n",
    "\n",
    "$$ \\Delta Q_{c,i} = \\frac{k_{i,c}}{m} - \\frac{2\\Sigma_{tot} \\; k_i}{(2m)^2} $$\n",
    "\n",
    "where $k_{i,c}$ denotes the sum of weights of edges from node $i$ to nodes in cluster $c$ and $\\Sigma_{tot}$ the sum of weights of edges incident to nodes in $c$.\n",
    "\n",
    "**Notes:**\n",
    "* It may be helpful to look at the lecture's slides on the topic again for understanding modularity and clustering algorithms.\n",
    "* This [website](https://towardsdatascience.com/louvain-algorithm-93fde589f58c) [2] may help you to understand more how the algorithm operates\n",
    "* You are free to use NetworkX function like Graph.degree and Graph.get_edge_data but not any modularity functions etc.\n",
    "* Your input graphs will always have a \"weight\" attribute with an assigned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Calculates the degree incident in node i\n",
    "def calc_ki(g, i):\n",
    "\n",
    "# TODO: Calculates the sum of the weights of all edges in the graph\n",
    "def calc_m(g):\n",
    "\n",
    "# TODO: Calculates the sum of the weights of all edges to nodes in com\n",
    "def calc_tot(g, com):\n",
    "\n",
    "# TODO: Calculates the sum of the weights of edges from node i to nodes in com\n",
    "# NOT SURE: it calculates A_{ij} from the first formula\n",
    "def calc_ki_in(g, i, com):\n",
    "\n",
    "# TODO: Calculates the delta Modularity with the formula given above\n",
    "# NOT SURE: It should be the [] part of the first formula\n",
    "def calc_deltaMm(g, i, com, m):\n",
    "\n",
    "# TODO: Calculates the sum of the weights of all intra-community edges in com\n",
    "# It calculates L_c in the simplified formula above\n",
    "def calc_lc(g, com):\n",
    "\n",
    "# TODO: Calculates the sum of degrees of the nodes in com\n",
    "# It calculates k_c in the simplified formula above\n",
    "def calc_kc(g, com):\n",
    "\n",
    "# TODO: Calculates the modularity with the formula given above\n",
    "def calc_mod(g, M_com, m):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Community Aggregation\n",
    "After finishing the first step, all nodes belonging to the same community are merged into a single node. This step also generates self-loops which are the sum of all links inside a given community, before being collapsed into one node.\n",
    "\n",
    "**Notes:**\n",
    "* You are free to implement everything on your own (as long as the result is correct), but a possible way to solve it is by working with a dictionary containing the new partition.\n",
    "* It is useful to add an attribute 'name' to the original graph's nodes in order to preserve the indices as they are reset when creating subgraphs.\n",
    "* When testing your function, you can use the function `plotGraph` defined below to get a visual verification.\n",
    "* `M_com` is a dictionary that contains the new community partition (e.g. M_com equals {'A': {'A', 'B'}, 'B': set(), 'C': set(), 'D': {'C', 'D'}, 'E': {'E', 'F'}, 'F': set()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGraph(g):\n",
    "    # Create positions of all nodes and save them\n",
    "    pos = nx.spring_layout(g)\n",
    "\n",
    "    # Draw the graph according to node positions\n",
    "    nx.draw(g_new, pos, with_labels=True)\n",
    "\n",
    "    # Create edge labels\n",
    "    labels = {e: str(g.get_edge_data(*e)) for e in g.edges}\n",
    "\n",
    "    # Draw edge labels according to node positions\n",
    "    nx.draw_networkx_edge_labels(g, pos, edge_labels=labels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeCommunities(g, M_com):\n",
    "    # TODO:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Louvain Loop\n",
    "The next step is to implement the core functionality. Determine the best partition based on modularity in the current graph and use your function `mergeCommunities` to merge accordingly. This is repeated until no improvement can be made. The final output is the graph with the best clustering and the corresponding modularity value.\n",
    "\n",
    "**Notes:**\n",
    "* It may be helpful to look at the lecture's slides on the topic again for understanding modularity and clustering algorithms.\n",
    "* Our test graph is taken from this [website](https://www.statworx.com/de/blog/community-detection-with-louvain-and-infomap/) [4] which once again explains the algorithm [3] and you can lookup the correct partition of said graph there.\n",
    "* For testing purposes you might want to use more than one graph. Remember to assign weights to your graphs edges as we did below on the Krackhardt Kite Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Krackhardt Kite Graph with assigned weight=1\n",
    "def krackhardt_graph():\n",
    "    g = nx.krackhardt_kite_graph()\n",
    "    for e in g.edges():\n",
    "        g[e[0]][e[1]][\"weight\"] = 1\n",
    "    return g\n",
    "\n",
    "# Test graph from linked website\n",
    "def test_graph():\n",
    "    g = nx.Graph()\n",
    "    g.add_node(\"A\")\n",
    "    g.add_node(\"B\")\n",
    "    g.add_node(\"C\")\n",
    "    g.add_node(\"D\")\n",
    "    g.add_node(\"E\")\n",
    "    g.add_node(\"F\")\n",
    "    g.add_edge(\"A\", \"B\", weight=5)\n",
    "    g.add_edge(\"A\", \"C\", weight=4)\n",
    "    g.add_edge(\"A\", \"E\", weight=1)\n",
    "    g.add_edge(\"B\", \"C\", weight=2)\n",
    "    g.add_edge(\"C\", \"D\", weight=7)\n",
    "    g.add_edge(\"D\", \"F\", weight=3)\n",
    "    g.add_edge(\"E\", \"F\", weight=8)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = test_graph()\n",
    "# Can be used for testing\n",
    "#g = krackhardt_graph() \n",
    "\n",
    "m = # TODO\n",
    "k = 0 # iteration number\n",
    "\n",
    "# Loop until no improvement can be made through merging\n",
    "while True:\n",
    "    V = g.nodes\n",
    "    M_com = {} # M_com is explained in sub task b)\n",
    "    nodes_dict = {} # A dictionary that contains all nodes as keys and tracks their respective community\n",
    "\n",
    "    # TODO: Initialize M_com and calculate mod_new\n",
    "    \n",
    "    \n",
    "    # Loop until no node movement/improvement can be made\n",
    "    while True:\n",
    "       \n",
    "        mod = mod_new\n",
    "\n",
    "        M_com_old = copy.deepcopy(M_com)\n",
    "        v_random = random.sample(V, k=len(V))\n",
    "        \n",
    "        # TODO: Loop over all nods and communities in order to find the best partition\n",
    "    \n",
    "    \n",
    "    # TODO: Merge the communities with your function if an improvement is made\n",
    "    \n",
    "    \n",
    "print(\"End result\")\n",
    "print(\"Modularity: \" + str(mod))\n",
    "plotGraph(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Clustering Comparison \n",
    "Now you have implemented a clustering algorithm yourself, it is time to compare performances of different clustering approaches. It is your task to compare algorithms \"K-means\" and \"Gaussian Mixture Models\" and evaluate their results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) The dataset\n",
    "First we create a dataset. The dataset contains samples from two overlapping Gaussian distributions as well as a uniform distribution. You can change the distributions to different values for testing and answering questions below but besides that you do not have to edit this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "# Create dataset with 2 Gaussian distributions and a uniform distribution\n",
    "def create_dataset(datapoints=3000):\n",
    "    \n",
    "    # Create dataset consisting only of zeros\n",
    "    data = np.zeros([datapoints, 3])\n",
    "    dist_size = datapoints//3\n",
    "\n",
    "    # Create 2 Gaussian distributions and one uniform distribution\n",
    "    gaussian_dist1 = npr.multivariate_normal(mean=[-1, 1], cov=[[2, 1],[1, 2]], size=(dist_size))\n",
    "    gaussian_dist2 = npr.multivariate_normal(mean=[1, 5], cov=[[3, 1],[2, 1]], size=(dist_size))\n",
    "    uniform_dist = npr.uniform([5, -3], [10, 5], size=(dist_size, 2))\n",
    "\n",
    "    # Merge all distributions into one dataset\n",
    "    data[:dist_size, :2] = gaussian_dist1\n",
    "    data[:dist_size, 2] = np.asarray([0]*dist_size)\n",
    "    data[dist_size:2*dist_size, :2] = gaussian_dist2\n",
    "    data[dist_size:2*dist_size, 2] = np.asarray([1]*dist_size)\n",
    "    data[2*dist_size:, :2] = uniform_dist\n",
    "    data[2*dist_size:, 2] = np.asarray([2]*dist_size)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Clustering\n",
    "With a dataset in your hands, you can now define a K-means and GMM model that compute clusterings. Use the same number of max iterations for both, so they are comparable to each other. \n",
    "\n",
    "First you have to initialize the models and then compute clusterings on our our data.\n",
    "\n",
    "**Notes:**\n",
    "* You do not have to code anything yourself, just use the pre-defined functions.\n",
    "* Think about an optimal number of clusters for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "data = create_dataset(3000)\n",
    "\n",
    "# TODO: Initialize K-Means and Gaussian Mixture models\n",
    "\n",
    "\n",
    "# TODO: Fit and predict the data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Plotting and evaluation\n",
    "Finally you can plot the results. You can change different parameters in the code blocks above to answer the following questions:\n",
    "\n",
    "* What kind of effect does the amount of datapoints have?\n",
    "* Describe the kind of error (shape) K-means/GMM produce!\n",
    "* Compare the results of K-means and GMM with different parameter settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations here**  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "**Note**: Depending on the amount of datapoints you have, it may take a while to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [\"r\", \"b\", \"g\"]\n",
    "\n",
    "# Color the datapoints according to the kmeans prediction\n",
    "\n",
    "for i, point in enumerate(data):\n",
    "    plt.scatter(point[0],point[1],color=colors[y_kmeans[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GMM clustering\n",
    "**Note**: Depending on the amount of datapoints you have, it may take a while to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = [\"r\", \"b\", \"g\"]\n",
    "\n",
    "# Color the datapoints according to the gmm prediction\n",
    "\n",
    "for i, point in enumerate(data):\n",
    "    plt.scatter(point[0],point[1],color=colors[y_gmm[i]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html\n",
    "<br>[2] https://towardsdatascience.com/louvain-algorithm-93fde589f58c\n",
    "<br>[3] https://miro.medium.com/max/2400/1*gglKE-X25TSwtw9kG0D5vg.png\n",
    "<br>[4] https://www.statworx.com/de/blog/community-detection-with-louvain-and-infomap/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
